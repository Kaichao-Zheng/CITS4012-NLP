{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOsq3No1kRC-"
   },
   "source": [
    "# 2025 CITS4012 Group XX Assignment\n",
    "\n",
    "*Make sure you change the file name with your student id.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THS1ppSwkgz4"
   },
   "source": [
    "# 0. Readme\n",
    "这是Project 1的模板，我直接拿来用了\n",
    "\n",
    "仅需注重代码风格及可视化，让marker改起来舒服就行，分数低不了的\n",
    "* 例如：多抄Lab的技术栈，多引用学界的基石论文，marker自然乐意审自己熟悉的技术。e.g., WordCloud Visualization\n",
    "\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object *Oriented* Programming style, please put those the bottom of this ipynb file*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyfRpsNP7LAM"
   },
   "source": [
    "## 0.1 File Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLyUKhFG7Oof"
   },
   "source": [
    "```\n",
    "/content/drive/MyDrive/path_to_your_folder\n",
    "├── CITS4012_YourGroupID.ipynb\n",
    "├── sample\n",
    "├── sample\n",
    "├── sample\n",
    "├── sample\n",
    "├── sample\n",
    "├── sample\n",
    "├── test.json\n",
    "├── train.json\n",
    "└── validation.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LPgCx5t-0f8"
   },
   "source": [
    "## 0.2 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8keiMLU-9WY"
   },
   "source": [
    "### Step 1 - Check current Python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHWSodks-_pn",
    "outputId": "429a6e98-2573-4b73-95ae-773c6ea9cdee"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOMqLIfz_CAs"
   },
   "source": [
    "### Step 2 - Install dependencies\n",
    "\n",
    "> NOTE:\n",
    ">\n",
    "> In Google Drive, an ERROR may occur due to incompatibility of the latest versions of `numpy` and `scipy`.\n",
    ">\n",
    "> Simply **restart the runtime** to use the newly downgraded versions.\n",
    ">\n",
    "> (In other words, run this code snippet twice to make sure all requirements are satisfied.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XsKNGCKj_DxD",
    "outputId": "d0bc86e5-3e3a-403e-971e-79c67500e0bc"
   },
   "outputs": [],
   "source": [
    "%pip install word2number\n",
    "%pip install contractions\n",
    "%pip install nltk\n",
    "%pip install pandas\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rINt3L_9BtYI"
   },
   "source": [
    "### Step 3 - Mount source files\n",
    "\n",
    "This step assumes you are running in a Google Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThxhGtqzCcr_"
   },
   "outputs": [],
   "source": [
    "# Define your Google Drive folder path\n",
    "my_folder = \"MyDrive/Colab Notebooks/path_to_your_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCe4ti1DCia0",
    "outputId": "a28a952a-47d3-452c-e04d-debb51928319"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path setups\n",
    "import os\n",
    "folder_path = \"/content/drive/\" + my_folder\n",
    "\n",
    "test_file = os.path.join(folder_path, \"test.json\")\n",
    "train_file = os.path.join(folder_path, \"train.json\")\n",
    "val_file = os.path.join(folder_path, \"validation.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0at-kJs67gh"
   },
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcBQf2U37lwG"
   },
   "source": [
    "We implemented three substantially different model architectures:\n",
    "\n",
    "* [The vanilla RNN-based encoder-decoder](#scrollTo=67TQJgOJ_lF1)\n",
    "\n",
    "* [The Bi-LSTM encoder-decoder with XXX-attention in different positions](#scrollTo=BhSE5ON4_r0C)\n",
    "\n",
    "* [The vanilla Transformer with self-attention (no RNN)](#scrollTo=E4w_n2P2_xxm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tgj4JarkjAY"
   },
   "source": [
    "# 2. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eylhXsB4Fb_V"
   },
   "source": [
    "## 2.1 Load JSON files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vF0lwUqfkj6p",
    "outputId": "4bc98a19-47de-4bcc-f25d-af04a1ddc95b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Working dataframes\n",
    "train_df = pd.read_json(train_file)\n",
    "val_df = pd.read_json(val_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "\n",
    "# Keep original copies\n",
    "source_train_df = train_df.copy()\n",
    "source_val_df = val_df.copy()\n",
    "source_test_df = test_df.copy()\n",
    "\n",
    "# Sneak peek\n",
    "pd.set_option(\"display.max_colwidth\", 30)\n",
    "\n",
    "print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "pd.reset_option(\"display.max_colwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9ibjBESaH08"
   },
   "source": [
    "## 2.2 Data cleansing\n",
    "\n",
    "原则上，我们只知道训练集数据，不该为验证集/测试集的特点设计数据清洗的代码\n",
    "\n",
    "TODO\n",
    "\n",
    "1. 洗 146/427， 147\n",
    "2. 未来处理Noise2时的抉择:\n",
    "    * 拆段成句，打乱原始下标\n",
    "    * 干扰Padding效率，因为每个batch的所有sent都得补齐到max_sent_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzfrlfEJgpZ_"
   },
   "source": [
    "Assumption\n",
    "* The value of `label` is binary, either \"neutral\" or \"entails\".\n",
    "\n",
    "Compromises\n",
    "* Ignore syntactic errors and semantic errors.\n",
    "* Apply unified premises rules on hypothesis.\n",
    "\n",
    "Handled Issues\n",
    "| No. | Description | Examples |\n",
    "|----------|-------------|----------|\n",
    "| Issue 1  | HTML/XML tags with ID pattern | train premise 78, 270, 319, ... |\n",
    "| Issue 2  | Non-linguistic long separators | train premise 1, 382, 697, ... |\n",
    "| Issue 3  | Library Catalog ID | train premise 147, 270, 319, ... |\n",
    "| Issue 4  | Duplicate consecutive phrases | train premise 78, 87, 564, ... |\n",
    "| Issue 5  | Duplicated whitespaces | train premise 123, 193, 259, ... |\n",
    "| Issue 6  | Spaces before punctuations, except '!' and '?' | train premise 3, 333, 6280 |\n",
    "\n",
    "Kept Noises\n",
    "\n",
    "| No. | Description | Noise | Non-Noise |\n",
    "|----------|-------------|----------|--------------|\n",
    "| Noise 1  | Instructional prompt words | train premise 3, 61, 319, ... | train premise 16, 24, 61, ... |\n",
    "| Noise 2  | Concatenated sentences | train premise 270, 537, 608, ... |  |\n",
    "| Noise 3  | Numbered markers | train premise 270, 537, 608, ... | train premise 1546, 2068, ... |\n",
    "| Noise 4  | Misplaced `label` values | train premise 270, 537, 606, ... | train premise 1683, 2068, ... |\n",
    "\n",
    "Limitation\n",
    "\n",
    "The difference viewer automatically escapes HTML entities, which might overlook HTML noise. (E.g., &amp;quot; in train premise 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "id": "w1jUqkRJadr_",
    "outputId": "d477f17a-bec0-4401-c001-32f4387e0b89"
   },
   "outputs": [],
   "source": [
    "import re, html\n",
    "from difflib import SequenceMatcher\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def cleanse(df):\n",
    "  df = df.copy()\n",
    "\n",
    "  ID_PATTERN = r\"\\b[A-Za-z]?(?:\\d{6,}|[A-Za-z0-9]{8,})(?:-[A-Za-z0-9]{2,})+\\b\"\n",
    "  REPEAT_PATTERN = r\"\\b((?:\\w+\\s+){0,2}\\w+)( \\1\\b)+\"\n",
    "\n",
    "  for col in [\"premise\", \"hypothesis\"]:\n",
    "\n",
    "    # issue 1: HTML/XML tags with ID pattern\n",
    "    df[col] = df[col].apply(html.unescape)\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"<[^>]*>\", \" \", x))\n",
    "    df[col] = df[col].apply(lambda x: re.sub(ID_PATTERN, \"[id]\", x))\n",
    "\n",
    "    # issue 2: non-linguistic long separators\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"[-=*_~$]{3,}\", \" \", x))\n",
    "\n",
    "    # issue 3: duplicate continuout words\n",
    "    df[col] = df[col].apply(lambda x: re.sub(REPEAT_PATTERN, r\"\\1\", x))\n",
    "\n",
    "    # issue 4: duplicate whitespaces\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
    "\n",
    "    # issue 5: spaces before punctuations\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"\\s+([.,;:])\", r\"\\1\", x))\n",
    "\n",
    "  return df\n",
    "\n",
    "def verify_print(source_df, cleaned_df, tag, num, col, index, kept=False):\n",
    "  status = \"Kept \" if kept else \"Fixed\"\n",
    "  print(f\"{tag} {num}:\\t{source_df[col][index]}\")\n",
    "  print(f\"{status} {num}:\\t{cleaned_df[col][index]}\")\n",
    "  print()\n",
    "\n",
    "# You can enable dark mode by replacing default \"light\" here\n",
    "def verify_diff(source_df, cleaned_df, tag, num, col, index, kept=False, theme=\"dark\"):\n",
    "    if theme == \"light\":\n",
    "      bg = \"#fafafa\"; fg = \"#000000\"\n",
    "      del_bg = \"#ffdddd\"; ins_bg = \"#ddffdd\"\n",
    "      del_fg = \"#aa0000\"; ins_fg = \"#006600\"\n",
    "    else:\n",
    "      bg = \"#1e1e1e\"; fg = \"#e0e0e0\"\n",
    "      del_bg = \"#662222\"; ins_bg = \"#224422\"\n",
    "      del_fg = \"#ff9999\"; ins_fg = \"#99ff99\"\n",
    "\n",
    "    status = \"Kept\" if kept else \"Fixed\"\n",
    "\n",
    "    original = source_df[col][index]\n",
    "    cleaned = cleaned_df[col][index]\n",
    "\n",
    "    matcher = SequenceMatcher(None, original, cleaned)\n",
    "\n",
    "    html_original = f\"\"\"\n",
    "    <div style='margin-bottom: 10px; background-color:{bg}; color:{fg}; white-space: nowrap; font-family: monospace; padding: 5px; border-radius:4px;'>\n",
    "    <b>{tag} {num}:</b> \"\"\"\n",
    "    html_cleaned = f\"\"\"\n",
    "    <div style='margin-bottom: 10px; background-color:{bg}; color:{fg}; white-space: nowrap; font-family: monospace; padding: 5px; border-radius:4px;'>\n",
    "    <b>{status} {num}:</b> \"\"\"\n",
    "\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "      orig_text = original[i1:i2].replace(\" \", \"&nbsp;\")\n",
    "      clean_text = cleaned[j1:j2].replace(\" \", \"&nbsp;\")\n",
    "\n",
    "      if tag == 'equal':\n",
    "        html_original += orig_text\n",
    "        html_cleaned += clean_text\n",
    "      elif tag == 'delete':\n",
    "        html_original += f\"<span style='background-color:{del_bg}; color:{del_fg};'>{orig_text}</span>\"\n",
    "      elif tag == 'insert':\n",
    "        html_cleaned += f\"<span style='background-color:{ins_bg}; color:{ins_fg};'>{clean_text}</span>\"\n",
    "      elif tag == 'replace':\n",
    "        html_original += f\"<span style='background-color:{del_bg}; color:{del_fg};'>{orig_text}</span>\"\n",
    "        html_cleaned += f\"<span style='background-color:{ins_bg}; color:{ins_fg};'>{clean_text}</span>\"\n",
    "\n",
    "    html_original += \"</div>\"\n",
    "    html_cleaned += \"</div>\"\n",
    "\n",
    "    display(HTML(html_original + html_cleaned + \"<hr/>\"))\n",
    "\n",
    "# Clenasing\n",
    "train_df = cleanse(train_df)\n",
    "val_df = cleanse(val_df)\n",
    "test_df = cleanse(test_df)\n",
    "\n",
    "# Keep cleaned copies\n",
    "cleaned_train_df = train_df.copy()\n",
    "cleaned_val_df = val_df.copy()\n",
    "cleaned_test_df = test_df.copy()\n",
    "\n",
    "# Verification\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=1, col=\"premise\", index=319)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=2, col=\"premise\", index=1)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=3, col=\"premise\", index=87)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=4, col=\"premise\", index=123)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=5, col=\"premise\", index=3)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=5, col=\"premise\", index=6280, kept=True)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=5, col=\"premise\", index=333, kept=True)\n",
    "\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=\"Hybrid\", col=\"premise\", index=270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDOTLdmDKwfy"
   },
   "source": [
    "## 2.3 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rc0V-G-Ei8lr",
    "outputId": "f5fb5304-b4b8-4b15-889c-95210299a6a4"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "gAEbhAKoKyEW",
    "outputId": "f421770d-b2a1-4db3-a022-ed6d46156fef"
   },
   "outputs": [],
   "source": [
    "import re, contractions\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from word2number import w2n\n",
    "from contractions import fix as expand_contractions\n",
    "\n",
    "sww = stopwords.words('english')\n",
    "\n",
    "def word2num(text):\n",
    "  def convert(match):\n",
    "    word = match.group(0)\n",
    "    try:\n",
    "      return f\" {w2n.word_to_num(word)} \"\n",
    "    except:\n",
    "      return f\" {word} \"\n",
    "  text = re.sub(\n",
    "    r'\\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten|'\n",
    "    r'eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|'\n",
    "    r'eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|'\n",
    "    r'eighty|ninety|hundred|thousand|million|billion|and|[- ])+\\b',\n",
    "    convert, text)\n",
    "  return ' '.join(text.split())\n",
    "\n",
    "def normalize(df):\n",
    "  for col in [\"premise\", \"hypothesis\"]:\n",
    "\n",
    "    # rule 1: lowercase\n",
    "    df[col] = df[col].str.lower()\n",
    "\n",
    "    # rule 2: expand contraction\n",
    "    df[col] = df[col].apply(expand_contractions)\n",
    "\n",
    "    # rule -1: remove stopwords\n",
    "    # df[col] = df[col].apply(lambda x: \" \".join([w for w in word_tokenize(x) if w.lower() not in sww]))\n",
    "\n",
    "    # rule -2: remove bracketed content\n",
    "    # df[col] = df[col].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x))\n",
    "\n",
    "    # rule -3: symbolize linguistic numbers\n",
    "    # df[col] = df[col].apply(word2num)\n",
    "\n",
    "  return df\n",
    "\n",
    "# Normalization\n",
    "train_df = normalize(train_df)\n",
    "\n",
    "# Verification\n",
    "verify_diff(cleaned_train_df, train_df, tag=\"Rule\", num=\"1\", col=\"premise\", index=1)\n",
    "verify_diff(cleaned_train_df, train_df, tag=\"Rule\", num=\"2\", col=\"premise\", index=420)\n",
    "verify_diff(cleaned_train_df, train_df, tag=\"Rule\", num=\"2\", col=\"hypothesis\", index=8, kept=True)\n",
    "\n",
    "# Inactivate rules\n",
    "# verify_diff(cleaned_train_df, train_df, tag=\"Rule\", num=-1, col=\"premise\", index=1, kept=True)\n",
    "# verify_diff(cleaned_train_df, train_df, tag=\"Rule\", num=-2, col=\"premise\", index=8, kept=True)\n",
    "# verify_diff(cleaned_train_df, train_df, tag=\"Rule\", num=-3, col=\"premise\", index=147, kept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adtX9t6XKyTd"
   },
   "source": [
    "## 2.4 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mX8ghj_AK0uG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDxZSK20klWr"
   },
   "source": [
    "# 3. Word Embedding Construction\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fleoeIBDkr5i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZJlo-z8k0ai"
   },
   "source": [
    "# 4. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67TQJgOJ_lF1"
   },
   "source": [
    "## 4.1 The vanilla RNN-based encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRXewgTnk6sB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhSE5ON4_r0C"
   },
   "source": [
    "## 4.2 The Bi-LSTM encoder-decoder with XXX-attention in different positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b14s1AoN_wNC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4w_n2P2_xxm"
   },
   "source": [
    "## 4.3 The vanilla Transformer with self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpZNRYhe_1ae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GU_rQZJk87q"
   },
   "source": [
    "# 5. Performance Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L11vnP-lk_ZH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zvj4_3Qak_x7"
   },
   "source": [
    "# 6. Interactive Inference Colab Form\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMJN1Dx1lES9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
