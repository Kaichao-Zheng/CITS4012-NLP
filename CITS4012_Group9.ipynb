{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaichao-Zheng/CITS4012-NLP/blob/main/CITS4012_Group9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOsq3No1kRC-"
      },
      "source": [
        "# 2025 CITS4012 Group 9 Assignment\n",
        "\n",
        "**Collaborators**\n",
        "\n",
        "| Uni ID   | Student Name  | GitHub Username                                   |\n",
        "| -------- | ------------- | ------------------------------------------------- |\n",
        "| 24141207 | Kaichao Zheng | [Kaichao-Zheng](https://github.com/Kaichao-Zheng) |\n",
        "| 24645175 | Ziqi Meng     | [jiongge39](https://github.com/jiongge39)         |\n",
        "| 23998001 | Yanglei Yuan  | [LeoYuan0225](https://github.com/LeoYuan0225)     |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THS1ppSwkgz4"
      },
      "source": [
        "# Readme\n",
        "这是Project 1的模板，我直接拿来用了\n",
        "\n",
        "仅需注重代码风格及可视化，让marker改起来舒服就行，分数低不了的\n",
        "* 例如：多抄Lab的技术栈，多引用学界的基石论文，marker自然乐意审自己熟悉的技术。e.g., WordCloud Visualization\n",
        "\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object *Oriented* Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LPgCx5t-0f8"
      },
      "source": [
        "# 0. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTV2-AYuV_lx"
      },
      "source": [
        "> NOTE:\n",
        ">\n",
        "> In Google Colab, an ERROR would occur due to incompatibility of the latest versions of `numpy` and `scipy`.\n",
        ">\n",
        "> Simply **restart the runtime** to use the newly downgraded versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsKNGCKj_DxD"
      },
      "outputs": [],
      "source": [
        "%pip install word2number\n",
        "%pip install contractions\n",
        "%pip install nltk\n",
        "%pip install pandas\n",
        "%pip install gensim\n",
        "%pip install matplotlib\n",
        "%pip install wordcloud\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0at-kJs67gh"
      },
      "source": [
        "# 1. Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcBQf2U37lwG"
      },
      "source": [
        "We implemented three substantially different model architectures:\n",
        "\n",
        "* [The Vanilla Bi-LSTM NLI Classifier](#scrollTo=67TQJgOJ_lF1)\n",
        "\n",
        "* [Model 2](#scrollTo=BhSE5ON4_r0C)\n",
        "\n",
        "* [Model 3](#scrollTo=E4w_n2P2_xxm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tgj4JarkjAY"
      },
      "source": [
        "# 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eylhXsB4Fb_V"
      },
      "source": [
        "## 2.1 Load JSON files from GitHub\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZlBU0GAUiTD"
      },
      "outputs": [],
      "source": [
        "# Define JSON dataset paths\n",
        "base_url = \"https://raw.githubusercontent.com/Kaichao-Zheng/CITS4012-NLP/main/\"\n",
        "\n",
        "train_file = base_url + \"train.json\"\n",
        "val_file = base_url + \"validation.json\"\n",
        "test_file = base_url + \"test.json\"\n",
        "\n",
        "# Quick check\n",
        "print(\"✅ Dataset URLs:\")\n",
        "print(\"Train:\\t\", train_file)\n",
        "print(\"Val:\\t\", val_file)\n",
        "print(\"Test:\\t\", test_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF0lwUqfkj6p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Working dataframes\n",
        "train_df = pd.read_json(train_file)\n",
        "val_df = pd.read_json(val_file)\n",
        "test_df = pd.read_json(test_file)\n",
        "\n",
        "# Keep original copies\n",
        "source_train_df = train_df.copy()\n",
        "source_val_df = val_df.copy()\n",
        "source_test_df = test_df.copy()\n",
        "\n",
        "# Sneak peek\n",
        "pd.set_option(\"display.max_colwidth\", 30)\n",
        "\n",
        "print(train_df.head())\n",
        "# print(val_df.head())\n",
        "# print(test_df.head())\n",
        "\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Val size:\", len(val_df))\n",
        "print(\"Test size:\", len(test_df))\n",
        "\n",
        "pd.reset_option(\"display.max_colwidth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJZtrMRPnYld"
      },
      "source": [
        "## 2.2 Define difference viewer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7PZFNNUncY7"
      },
      "outputs": [],
      "source": [
        "import html\n",
        "from difflib import SequenceMatcher\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def verify_print(source_df, cleaned_df, tag, num, col, idx_src ,idx_now, kept=False):\n",
        "  status = \"Kept \" if kept else \"Reindexed\"\n",
        "  print(f\"{tag} {num}:\\tidx_src: {idx_src}\\t{source_df[col][idx_src]}\")\n",
        "  print(f\"{status} {num}:\\tidx_now: {idx_now}\\t{cleaned_df[col][idx_now]}\")\n",
        "  print()\n",
        "\n",
        "# You can enable dark mode by replacing default \"light\" here\n",
        "def verify_diff(source_df, cleaned_df, tag, num, col, index, kept=False, theme=\"dark\"):\n",
        "  if theme == \"light\":\n",
        "    bg = \"#fafafa\"; fg = \"#000000\"\n",
        "    del_bg = \"#ffdddd\"; ins_bg = \"#ddffdd\"\n",
        "    del_fg = \"#aa0000\"; ins_fg = \"#006600\"\n",
        "  else:\n",
        "    bg = \"#1e1e1e\"; fg = \"#e0e0e0\"\n",
        "    del_bg = \"#662222\"; ins_bg = \"#224422\"\n",
        "    del_fg = \"#ff9999\"; ins_fg = \"#99ff99\"\n",
        "\n",
        "  status = \"Kept\" if kept else \"Fixed\"\n",
        "\n",
        "  original = source_df[col][index]\n",
        "  cleaned = cleaned_df[col][index]\n",
        "\n",
        "  matcher = SequenceMatcher(None, original, cleaned)\n",
        "\n",
        "  html_original = f\"\"\"\n",
        "  <div style='margin-bottom: 10px; background-color:{bg}; color:{fg}; white-space: nowrap; font-family: monospace; padding: 5px; border-radius:4px;'>\n",
        "  <b>{tag} {num}:</b> \"\"\"\n",
        "  html_cleaned = f\"\"\"\n",
        "  <div style='margin-bottom: 10px; background-color:{bg}; color:{fg}; white-space: nowrap; font-family: monospace; padding: 5px; border-radius:4px;'>\n",
        "  <b>{status} {num}:</b> \"\"\"\n",
        "\n",
        "  for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
        "    orig_text = original[i1:i2].replace(\" \", \"&nbsp;\")\n",
        "    clean_text = cleaned[j1:j2].replace(\" \", \"&nbsp;\")\n",
        "\n",
        "    if tag == 'equal':\n",
        "      html_original += orig_text\n",
        "      html_cleaned += clean_text\n",
        "    elif tag == 'delete':\n",
        "      html_original += f\"<span style='background-color:{del_bg}; color:{del_fg};'>{orig_text}</span>\"\n",
        "    elif tag == 'insert':\n",
        "      html_cleaned += f\"<span style='background-color:{ins_bg}; color:{ins_fg};'>{clean_text}</span>\"\n",
        "    elif tag == 'replace':\n",
        "      html_original += f\"<span style='background-color:{del_bg}; color:{del_fg};'>{orig_text}</span>\"\n",
        "      html_cleaned += f\"<span style='background-color:{ins_bg}; color:{ins_fg};'>{clean_text}</span>\"\n",
        "\n",
        "  html_original += \"</div>\"\n",
        "  html_cleaned += \"</div>\"\n",
        "\n",
        "  display(HTML(html_original + html_cleaned + \"<hr/>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ibjBESaH08"
      },
      "source": [
        "## 2.3 Data cleansing\n",
        "\n",
        "In principle, the data cleansing rules are derived solely from the training set to prevent information leakage.\n",
        "\n",
        "This design intentionally preserves out-of-vocabulary (OOV) cases in the validation and test sets, which are represented by \\<UNK> tag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzfrlfEJgpZ_"
      },
      "source": [
        "Assumption\n",
        "* The value of `label` is binary, either \"neutral\" or \"entails\".\n",
        "\n",
        "Compromises\n",
        "* Ignore syntactic errors and semantic errors.\n",
        "* Apply unified premises rules on hypothesis.\n",
        "\n",
        "Handled Issues\n",
        "| No. | Description | Examples |\n",
        "|----------|-------------|-----------|\n",
        "| Issue 1 | HTML/XML tags with ID pattern | train premise 78, 270, 319, ... |\n",
        "| Issue 2 | Non-linguistic long/pure separators | train premise 1, 382, 385, ... |\n",
        "| Issue 3 | Duplicate consecutive phrases | train premise 78, 87, 564, ... |\n",
        "| Issue 4 | Single-word sentences  | train premise 146, 181, 427, ... |\n",
        "| Issue 5 | Duplicated whitespaces | train premise 123, 193, 259, ... |\n",
        "| Issue 6 | Spaces before punctuations, except '!' and '?' | train premise 3, 333, 6280 |\n",
        "| Issue 7 | Premise with long concatenated sentences | train premise 270, 537, 608, ... |\n",
        "\n",
        "Kept Noises\n",
        "\n",
        "| No. | Description | Noise | Non-Noise |\n",
        "|----------|-------------|----------|--------------|\n",
        "| Noise 1 | Instructional prompt words | train premise 3, 61, 319, ... | train premise 16, 24, 61, ... |\n",
        "| Noise 2 | Numbered markers | train premise 270, 537, 608, ... | train premise 1546, 2068, ... |\n",
        "| Noise 3 | Misplaced `label` values | train premise 270, 537, 606, ... | train premise 1683, 2068, ... |\n",
        "| Noise 4 | Metadata prefixes | train premise 32, 230, 482, ... |  |\n",
        "| Noise 5 | Isolated single symbols | train premise 60, 1185 | comparison operators |\n",
        "\n",
        "\n",
        "Limitation\n",
        "\n",
        "The difference viewer automatically escapes HTML entities, which might overlook HTML noise. (E.g., &amp;quot; in train premise 420)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1jUqkRJadr_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def cleanse(df):\n",
        "  df = df.copy()\n",
        "\n",
        "  ID_PATTERN = r\"\\b[A-Za-z]?(?:\\d{6,}|[A-Za-z0-9]{8,})(?:-[A-Za-z0-9]{2,})+\\b\"\n",
        "  REPEAT_PATTERN = r\"\\b((?:\\w+\\s+){0,2}\\w+)( \\1\\b)+\"\n",
        "\n",
        "  for col in [\"premise\", \"hypothesis\"]:\n",
        "\n",
        "    # pre-trim: two-tailed whitespaces\n",
        "    df[col] = df[col].apply(lambda x: x.strip())\n",
        "\n",
        "    # issue 1: HTML/XML tags with ID pattern\n",
        "    df[col] = df[col].apply(html.unescape)\n",
        "    df[col] = df[col].apply(lambda x: re.sub(r\"<[^>]*>\", \" \", x))\n",
        "    df[col] = df[col].apply(lambda x: re.sub(ID_PATTERN, \" \", x))\n",
        "\n",
        "    # issue 2: non-linguistic long/pure separators\n",
        "    df[col] = df[col].apply(lambda x: re.sub(r\"[-=*_~$]{3,}\", \" \", x))\n",
        "    df[col] = df[col].apply(lambda x: \"\" if re.fullmatch(r\"[\\W_]+\", x.strip()) else x)\n",
        "\n",
        "    # issue 3: duplicate consecutive phrases\n",
        "    df[col] = df[col].apply(lambda x: re.sub(REPEAT_PATTERN, r\"\\1\", x))\n",
        "\n",
        "    # issue 4: single-word sentences\n",
        "    df[col] = df[col].apply(lambda x: \"\" if re.fullmatch(r\"(\\w+[.!?']?|[^\\w\\s]+)\", x.strip()) else x)\n",
        "\n",
        "    # issue 5: duplicate whitespaces\n",
        "    df[col] = df[col].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
        "\n",
        "    # issue 6: spaces before punctuations\n",
        "    df[col] = df[col].apply(lambda x: re.sub(r\"\\s+([.,;:])\", r\"\\1\", x))\n",
        "\n",
        "  return df\n",
        "\n",
        "# Clenasing\n",
        "train_df = cleanse(train_df)\n",
        "val_df = cleanse(val_df)\n",
        "test_df = cleanse(test_df)\n",
        "\n",
        "# Keep cleaned copies\n",
        "cleaned_train_df = train_df.copy()\n",
        "cleaned_val_df = val_df.copy()\n",
        "cleaned_test_df = test_df.copy()\n",
        "\n",
        "# Verification\n",
        "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=1, col=\"premise\", index=319)\n",
        "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=2, col=\"premise\", index=1)\n",
        "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=3, col=\"premise\", index=87)\n",
        "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=4, col=\"premise\", index=146)\n",
        "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=5, col=\"premise\", index=123)\n",
        "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=6, col=\"premise\", index=3)\n",
        "# verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=6, col=\"premise\", index=6280, kept=True)  # why except '?'\n",
        "# verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=6, col=\"premise\", index=333, kept=True)  # why except '!'\n",
        "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=\"Hybrid\", col=\"premise\", index=270)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDOTLdmDKwfy"
      },
      "source": [
        "## 2.4 Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc0V-G-Ei8lr"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "sww = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAEbhAKoKyEW"
      },
      "outputs": [],
      "source": [
        "import re, contractions\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from word2number import w2n\n",
        "from contractions import fix as expand_contractions\n",
        "\n",
        "def word2num(text):\n",
        "  def convert(match):\n",
        "    word = match.group(0)\n",
        "    try:\n",
        "      return f\" {w2n.word_to_num(word)} \"\n",
        "    except:\n",
        "      return f\" {word} \"\n",
        "  text = re.sub(\n",
        "    r'\\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten|'\n",
        "    r'eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|'\n",
        "    r'eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|'\n",
        "    r'eighty|ninety|hundred|thousand|million|billion|and|[- ])+\\b',\n",
        "    convert, text)\n",
        "  return ' '.join(text.split())\n",
        "\n",
        "def normalize(df):\n",
        "  df = df.copy()\n",
        "\n",
        "  for col in [\"premise\", \"hypothesis\"]:\n",
        "\n",
        "    # rule 1: lowercase\n",
        "    df[col] = df[col].str.lower()\n",
        "\n",
        "    # rule 2: expand contraction\n",
        "    df[col] = df[col].apply(expand_contractions)\n",
        "\n",
        "    # rule -1: remove stopwords\n",
        "    # df[col] = df[col].apply(lambda x: \" \".join([w for w in word_tokenize(x) if w.lower() not in sww]))\n",
        "\n",
        "    # rule -2: remove bracketed content\n",
        "    # df[col] = df[col].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x))\n",
        "\n",
        "    # rule -3: symbolize linguistic numbers\n",
        "    # df[col] = df[col].apply(word2num)\n",
        "\n",
        "  return df\n",
        "\n",
        "# Normalization\n",
        "train_df = normalize(train_df)\n",
        "val_df = normalize(val_df)\n",
        "test_df = normalize(test_df)\n",
        "\n",
        "# Keep normalized copies\n",
        "normalized_train_df = train_df.copy()\n",
        "normalized_val_df = val_df.copy()\n",
        "normalized_test_df = test_df.copy()\n",
        "\n",
        "# Verification\n",
        "verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=\"1\", col=\"premise\", index=1)\n",
        "verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=\"2\", col=\"premise\", index=420)\n",
        "verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=\"2\", col=\"hypothesis\", index=8, kept=True)\n",
        "\n",
        "# Inactivate rules\n",
        "# verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=-1, col=\"premise\", index=1, kept=True)\n",
        "# verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=-2, col=\"premise\", index=8, kept=True)\n",
        "# verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=-3, col=\"premise\", index=147, kept=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1j8PKb8i3zP"
      },
      "source": [
        "## 2.5 Reindexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzpo8vldi509"
      },
      "outputs": [],
      "source": [
        "def reindex(df):\n",
        "  df = df.copy()\n",
        "\n",
        "  for col in [\"premise\", \"hypothesis\"]:\n",
        "\n",
        "    # post-issue 4: remove rows with empty cell\n",
        "    df = df[df[col].astype(str).str.strip() != \"\"]\n",
        "\n",
        "  # issue 7: premise with long concatenated sentences\n",
        "  new_rows = []\n",
        "  for _, row in df.iterrows():\n",
        "    premise = str(row[\"premise\"]).strip()\n",
        "    hypothesis = str(row[\"hypothesis\"]).strip()\n",
        "    label = str(row[\"label\"]).strip()\n",
        "\n",
        "    if not premise:\n",
        "      continue\n",
        "\n",
        "    # split by \". \" or \"; \" to avoid 3.14\n",
        "    sentences = re.split(r'(?<!\\d)(?<=[.;])\\s+(?=[A-Za-z])', premise)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    for s in sentences:\n",
        "      new_rows.append({\n",
        "        \"premise\": s,\n",
        "        \"hypothesis\": hypothesis,\n",
        "        \"label\": label\n",
        "      })\n",
        "  df = pd.DataFrame(new_rows)\n",
        "\n",
        "  # reorder index\n",
        "  df = df.reset_index(drop=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "# Reindexing\n",
        "train_df = reindex(train_df)\n",
        "val_df = reindex(val_df)\n",
        "test_df = reindex(test_df)\n",
        "\n",
        "# Keep reindexed copies\n",
        "reindexed_train_df = train_df.copy()\n",
        "reindexed_val_df = val_df.copy()\n",
        "reindexed_test_df = test_df.copy()\n",
        "\n",
        "# Verification\n",
        "verify_print(normalized_train_df, reindexed_train_df, tag=\"Post-Issue\", num=4, col=\"premise\", idx_src=147, idx_now=158)\n",
        "verify_print(normalized_train_df, reindexed_train_df, tag=\"Issue\", num=7, col=\"premise\", idx_src=31, idx_now=31, kept=True)\n",
        "verify_print(normalized_train_df, reindexed_train_df, tag=\"Issue\", num=7, col=\"premise\", idx_src=270, idx_now=281)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adtX9t6XKyTd"
      },
      "source": [
        "## 2.6 Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX8ghj_AK0uG"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVPHolC0DPzA"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def protect_float(text: str) -> str:\n",
        "  return re.sub(r\"(?<=\\d)\\.(?=\\d)\", \"DOTTK\", text)\n",
        "\n",
        "def recover_float(tokens):\n",
        "  return [t.replace(\"DOTTK\", \".\") for t in tokens]\n",
        "\n",
        "def tokenize(df):\n",
        "  df = df.copy()\n",
        "  KEEP_SYMBOLS = set(\"=<>+-/*%!\")\n",
        "\n",
        "  for col in [\"premise\", \"hypothesis\"]:\n",
        "    # Remove punctuations\n",
        "    df[col] = df[col].apply(lambda x: protect_float(x))\n",
        "    df[col] = df[col].apply(lambda x: [t for t in word_tokenize(x) if t not in string.punctuation or t in KEEP_SYMBOLS])\n",
        "    df[col] = df[col].apply(lambda toks: recover_float(toks))\n",
        "\n",
        "  return df\n",
        "\n",
        "# Tokenization\n",
        "tokenized_train_df = tokenize(train_df)\n",
        "tokenized_val_df = tokenize(val_df)\n",
        "tokenized_test_df = tokenize(test_df)\n",
        "\n",
        "# Quick check\n",
        "preview_rows = pd.concat([\n",
        "  tokenized_train_df.head(),\n",
        "  tokenized_train_df.iloc[[6, 23, 34, 62, 369, 438, 2116]]  # concatenate indeces 6, 23, 32, 60, 319, 385, 1185\n",
        "])\n",
        "\n",
        "display(preview_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm9wewk0QTzH"
      },
      "source": [
        "## 2.7 Visualisation via Word Cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b48-JUWxRG28"
      },
      "source": [
        "We actually include stopwords in the tokenized dataframes for model training,\n",
        "\n",
        "but temporarily remove them here for visualisation purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0robldKbRWOK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords', quiet=True)\n",
        "sww = stopwords.words('english')\n",
        "\n",
        "for col in [\"premise\", \"hypothesis\"]:\n",
        "  tokens = []\n",
        "  # Remove punctuation, number, and stopword tokens temporarily\n",
        "  for toks in tokenized_train_df[col]:\n",
        "    tokens.extend([t.lower() for t in toks if t.isalpha() and t.lower() not in sww])\n",
        "  wordcloud = WordCloud(background_color=\"white\").generate(\" \".join(tokens))\n",
        "  plt.figure()\n",
        "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(f\"{col.capitalize()} Word Cloud (Train Set)\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDxZSK20klWr"
      },
      "source": [
        "# 3. Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoSbUcLGaURF"
      },
      "source": [
        "## 3.1 Token vocabulary\n",
        "\n",
        "The vocabulary only includes tokens in train set with `frequency >= 3`.\n",
        "\n",
        "Therefore, a token with low frequency will be regarded as \\<UNK>.\n",
        "\n",
        "E.g. the token \"6.39\" from the first premise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6MnwSgIVx58"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_vocab(df, min_freq=3):\n",
        "  counter = Counter()\n",
        "  for col in [\"premise\", \"hypothesis\"]:\n",
        "    for toks in df[col]:\n",
        "      counter.update(toks)\n",
        "\n",
        "  # special tokens\n",
        "  vocab = {\"<PAD>\": 0,\"<UNK>\": 1}\n",
        "  for token, freq in counter.items():\n",
        "    if freq >= min_freq and token not in vocab:\n",
        "      vocab[token] = len(vocab)\n",
        "\n",
        "  return vocab\n",
        "\n",
        "vocab = build_vocab(tokenized_train_df)\n",
        "print(\"Vocab size:\\t\", {len(vocab)})\n",
        "print(\"Sample vocab:\\t\", list(vocab.keys())[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVw81ZCbgjSh"
      },
      "source": [
        "## 3.3 One-hot key embedding\n",
        "\n",
        "Map each token to its one-hot key from the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg2wWyg-bV3r"
      },
      "outputs": [],
      "source": [
        "def word_to_index(df, vocab):\n",
        "  df = df.copy()\n",
        "\n",
        "  for col in [\"premise\", \"hypothesis\"]:\n",
        "    df[col] = df[col].apply(lambda toks: [vocab.get(w, vocab[\"<UNK>\"]) for w in toks])\n",
        "\n",
        "  label_map = {\"neutral\": 0, \"entails\": 1}\n",
        "  if \"label\" in df.columns:\n",
        "    df[\"label\"] = df[\"label\"].map(label_map).fillna(-1).astype(int)\n",
        "\n",
        "  return df\n",
        "\n",
        "# One-hot key embedding\n",
        "indexed_train_df = word_to_index(tokenized_train_df, vocab)\n",
        "indexed_val_df = word_to_index(tokenized_val_df, vocab)\n",
        "indexed_test_df = word_to_index(tokenized_test_df, vocab)\n",
        "\n",
        "display(indexed_train_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZJlo-z8k0ai"
      },
      "source": [
        "# 4. Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTHB84RcbOFo"
      },
      "source": [
        "## 4.1 Training Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtl8jwlgCpu3"
      },
      "source": [
        "### 4.1.1 Enable cuda GPU acceleration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9W2gHTTCyM2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyhnsNdv7kVj"
      },
      "source": [
        "### 4.1.2 Shared hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgNqexCv7fHf"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "shared = SimpleNamespace(\n",
        "  vocab_size = len(vocab),\n",
        "  embed_dim = 256,\n",
        "  hidden_dim = 256,\n",
        "  num_layers = 1,\n",
        "  dropout = 0.1,\n",
        "  num_classes = 2,      # \"neutral\": 0, \"entails\": 1\n",
        "  batch_size = 128,\n",
        "  learning_rate = 1e-3,\n",
        "  total_epoch = 20,\n",
        "  pad_idx = vocab[\"<PAD>\"],\n",
        "  unk_idx = vocab[\"<UNK>\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NWYbjTKb7RE"
      },
      "source": [
        "### 4.1.3 Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P_ZR8E1b-j4"
      },
      "outputs": [],
      "source": [
        "def pad(seq, max_len):\n",
        "  return seq + [shared.pad_idx] * (max_len - len(seq))\n",
        "\n",
        "def pad_seq(df):\n",
        "  df = df.copy()\n",
        "\n",
        "  max_prem = max(df[\"premise\"].apply(len))\n",
        "  max_hypo = max(df[\"hypothesis\"].apply(len))\n",
        "\n",
        "  df[\"premise\"] = df[\"premise\"].apply(lambda s: pad(s, max_prem))\n",
        "  df[\"hypothesis\"] = df[\"hypothesis\"].apply(lambda s: pad(s, max_hypo))\n",
        "\n",
        "  return df\n",
        "\n",
        "# Padding\n",
        "pad_train_df = pad_seq(indexed_train_df)\n",
        "pad_val_df = pad_seq(indexed_val_df)\n",
        "pad_test_df = pad_seq(indexed_test_df)\n",
        "\n",
        "# Verification\n",
        "pd.set_option(\"display.max_colwidth\", 60)\n",
        "display(pad_train_df.head())\n",
        "pd.reset_option(\"display.max_colwidth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Gm9F5Mb-0p"
      },
      "source": [
        "### 4.1.4 Batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6cJWrEacAhH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def make_batch(pad_df):\n",
        "  batch_size = shared.batch_size\n",
        "\n",
        "  premises_tensor = torch.tensor(pad_df[\"premise\"].tolist())\n",
        "  hypotheses_tensor = torch.tensor(pad_df[\"hypothesis\"].tolist())\n",
        "  labels_tensor = torch.tensor(pad_df[\"label\"].tolist())\n",
        "\n",
        "  dataset = TensorDataset(premises_tensor, hypotheses_tensor, labels_tensor)\n",
        "  loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return loader\n",
        "\n",
        "train_loader = make_batch(pad_train_df)\n",
        "val_loader = make_batch(pad_val_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67TQJgOJ_lF1"
      },
      "source": [
        "## 4.2 The Vanilla Bi-LSTM NLI Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaiNx53_Sxia"
      },
      "source": [
        "This model is inspired by the classic SNLI model proposed by [Bowman et al.](https://arxiv.org/abs/1508.05326)\n",
        "* Single recurrent (LSTM) layer\n",
        "* Dropout applied to encoders and classifier\n",
        "* The value of `label` is either \"neutral\" or \"entails\"\n",
        "* Validation set used for monitoring overfitting\n",
        "* Remains affected by the OOV limitation\n",
        "* No attention mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6msx2of6C8_U"
      },
      "source": [
        "### 4.2.1 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoCjPNp8C-b4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Vanilla_NLI_Model(nn.Module):\n",
        "  def __init__(self, shared):\n",
        "    super().__init__()\n",
        "\n",
        "    # Embedding layer\n",
        "    self.prem_embedding = nn.Embedding(shared.vocab_size, shared.embed_dim, padding_idx=shared.pad_idx) # discard <PAD>\n",
        "    self.hypo_embedding = nn.Embedding(shared.vocab_size, shared.embed_dim, padding_idx=shared.pad_idx) # discard <PAD>\n",
        "\n",
        "    # Encoding Layer\n",
        "    self.encoder = nn.LSTM(\n",
        "      input_size=shared.embed_dim,\n",
        "      hidden_size=shared.hidden_dim,\n",
        "      num_layers=shared.num_layers,                  # single recurrent layer\n",
        "      dropout=shared.dropout if shared.num_layers > 1 else 0.0, # disabled， dropout only applies between recurrent layers\n",
        "      bidirectional=True,                       # Bi-LSTM\n",
        "      batch_first=True                        # (B, T, E)\n",
        "    )\n",
        "\n",
        "    # Regularization to prevent overfitting\n",
        "    self.dropout_encoder = nn.Dropout(shared.dropout)\n",
        "\n",
        "    # Classification layer\n",
        "    feat_dim = 2 * (2 * shared.hidden_dim)            # (prem + hypo) × BiLSTM(2H each) = 4H\n",
        "    self.classifier = nn.Linear(feat_dim, shared.num_classes)    # simplest linear classifier\n",
        "\n",
        "  def forward(self, prem_ids, hypo_ids):\n",
        "    # Encode premise\n",
        "    prem_embed = self.prem_embedding(prem_ids)            # (B, T_p, E)\n",
        "    _, (prem_h, _) = self.encoder(prem_embed)           # return encoder_output, (prem_hidden_state, prem_cell_state)\n",
        "    # Concat two-direction final hidden states\n",
        "    prem_vec = torch.cat([prem_h[-2], prem_h[-1]], dim=1)     # (B, 2H)\n",
        "    prem_vec = self.dropout_encoder(prem_vec)            # apply dropout to concatenated premise hidden state\n",
        "\n",
        "    # Encode hypothesis\n",
        "    hypo_embed = self.hypo_embedding(hypo_ids)            # (B, T_h, E)\n",
        "    _, (hypo_h, _) = self.encoder(hypo_embed)           # return encoder_output, (hypo_hidden_state, hypo_cell_state)\n",
        "    # Concat two-direction final hidden states\n",
        "    hypo_vec = torch.cat([hypo_h[-2], hypo_h[-1]], dim=1)     # (B, 2H)\n",
        "    hypo_vec = self.dropout_encoder(hypo_vec)            # apply dropout to concatenated hypothesis hidden state\n",
        "\n",
        "    # Combine premise and hypothesis as unified input for classifier\n",
        "    combined = torch.cat([prem_vec, hypo_vec], dim=1)       # (B, 4H)\n",
        "    combined = self.dropout_encoder(combined)            # apply dropout to combined premise-hypothesis representation\n",
        "\n",
        "    # Classification\n",
        "    logits = self.classifier(combined)                # (B, shared.num_classes)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diVN3tW6qmTL"
      },
      "source": [
        "### 4.2.2 Training (3 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQqFb5Csqv03"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = Vanilla_NLI_Model(shared).to(device)\n",
        "criterion = nn.CrossEntropyLoss()      # combine LogSoftmax and NLLLoss\n",
        "optimizer = optim.Adam(model.parameters(), lr=shared.learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hdF7T0pBxU2"
      },
      "outputs": [],
      "source": [
        "train_epoch_losses = []\n",
        "val_epoch_losses = []\n",
        "\n",
        "for epoch in range(shared.total_epoch):\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "\n",
        "  # Training loop\n",
        "  for prem, hypo, label in train_loader:\n",
        "    prem, hypo, label = prem.to(device), hypo.to(device), label.to(device)\n",
        "\n",
        "    # Forward\n",
        "    logits = model(prem, hypo)\n",
        "    loss = criterion(logits, label)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Accumulate batch loss\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  train_avg_loss = train_loss / len(train_loader)\n",
        "  train_epoch_losses.append(train_avg_loss)\n",
        "\n",
        "  # Validation\n",
        "  model.eval()\n",
        "  val_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for prem, hypo, label in val_loader:\n",
        "      prem, hypo, label = prem.to(device), hypo.to(device), label.to(device)\n",
        "\n",
        "      # Forward\n",
        "      logits = model(prem, hypo)\n",
        "      loss = criterion(logits, label)\n",
        "\n",
        "      # Accumulate batch loss\n",
        "      val_loss += loss.item()\n",
        "\n",
        "  val_avg_loss = val_loss / len(val_loader)\n",
        "  val_epoch_losses.append(val_avg_loss)\n",
        "\n",
        "  print(f\"Epoch: {epoch+1}/{shared.total_epoch}\\tTrain Loss: {train_avg_loss:.4f}\\tVal Loss: {val_avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPkVjEM4LNu-"
      },
      "source": [
        "### 4.2.3 Loss curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hznRuf-PLKk9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epoch = range(1, shared.total_epoch + 1)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
        "\n",
        "# Training Loss\n",
        "axes[0].plot(epoch, train_epoch_losses, marker='o', color='blue', label='Train Loss')\n",
        "axes[0].set_title('Training Loss per Epoch')\n",
        "axes[0].set_xticks(epoch)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Average Loss')\n",
        "axes[0].grid(axis='y')\n",
        "\n",
        "# Validation Loss\n",
        "axes[1].plot(epoch, val_epoch_losses, marker='o', color='orange')\n",
        "axes[1].set_title('Validation Loss per Epoch')\n",
        "axes[1].set_xticks(epoch)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Average Loss')\n",
        "axes[1].grid(axis='y')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMb7H46HOLqG"
      },
      "source": [
        "### 4.2.4 Evaluation\n",
        "\n",
        "In summary, the model struggles to predict OOV words, regardless of how much content it has learned from the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hVAggi_P469"
      },
      "source": [
        "#### Training Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QWsqzGoSlMm"
      },
      "source": [
        "The training loss decreases rapidly and stabilizes around epoch 10, indicating that the model successfully fits the training data.\n",
        "\n",
        "The consistent convergence pattern suggests that our vanilla RNN encoder-decoder is capable of capturing the sequence patterns within the training distribution, even without advanced mechanisms such as attention or pre-trained embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YABoXw1mLYfT"
      },
      "source": [
        "#### Validation Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc59zJh0Siqz"
      },
      "source": [
        "The validation loss rises steadily throughout training, revealing a clear sign of overfitting.\n",
        "\n",
        "This outcome is expected. Since the word-to-index vocabulary was intentionally built only from the training set\n",
        "\n",
        "As a result, unseen or rare tokens are replaced by the \\<UNK> tag, leading to higher loss and reduced generalization capability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhSE5ON4_r0C"
      },
      "source": [
        "## 4.3 Bi-LSTM + Attention NLI classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.1 Model"
      ],
      "metadata": {
        "id": "GBxtLuj6IgKV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b14s1AoN_wNC"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# NLI BiLSTM with Attention (Classifier + Training Loop)\n",
        "# ============================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "class NLI_BiLSTM_Attention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.bilstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 8, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def attention_pool(self, lstm_out):\n",
        "        attn_weights = F.softmax(self.attn(lstm_out), dim=1)  # [batch, seq_len, 1]\n",
        "        context = torch.sum(attn_weights * lstm_out, dim=1)   # [batch, hidden_dim*2]\n",
        "        return context\n",
        "\n",
        "    def forward(self, premise, hypothesis):\n",
        "        # Encode premise\n",
        "        prem_embed = self.embedding(premise)\n",
        "        prem_out, _ = self.bilstm(prem_embed)\n",
        "        prem_vec = self.attention_pool(prem_out)\n",
        "\n",
        "        # Encode hypothesis\n",
        "        hyp_embed = self.embedding(hypothesis)\n",
        "        hyp_out, _ = self.bilstm(hyp_embed)\n",
        "        hyp_vec = self.attention_pool(hyp_out)\n",
        "\n",
        "        # Combine\n",
        "        combined = torch.cat([\n",
        "            prem_vec,\n",
        "            hyp_vec,\n",
        "            torch.abs(prem_vec - hyp_vec),\n",
        "            prem_vec * hyp_vec\n",
        "        ], dim=1)\n",
        "\n",
        "        logits = self.fc(combined)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.2 Label"
      ],
      "metadata": {
        "id": "TURS5Xn3Ivsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming label_mapping and shared are already defined\n",
        "# Define the number of classes from the label_mapping\n",
        "label_mapping = {\"neutral\": 0, \"entails\": 1}\n",
        "num_classes = len(label_mapping)\n",
        "model = NLI_BiLSTM_Attention(\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=shared.embed_dim,\n",
        "    hidden_dim=shared.hidden_dim,\n",
        "    num_classes=num_classes,\n",
        "    pad_idx=shared.pad_idx\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=shared.learning_rate)"
      ],
      "metadata": {
        "id": "_NQF3c-PsQ0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.3 Training"
      ],
      "metadata": {
        "id": "VD1M_6L7Iy1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(label_mapping)\n",
        "model = NLI_BiLSTM_Attention(\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=shared.embed_dim,\n",
        "    hidden_dim=shared.hidden_dim,\n",
        "    num_classes=num_classes,\n",
        "    pad_idx=shared.pad_idx\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=shared.learning_rate)\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(shared.total_epoch):\n",
        "    # ---------- TRAIN ----------\n",
        "    model.train()\n",
        "    running_loss, preds, labels_all = 0, [], []\n",
        "    for premise, hypothesis_in, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{shared.total_epoch}\"):\n",
        "        premise, hypothesis_in, labels = premise.to(device), hypothesis_in.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(premise, hypothesis_in)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "        labels_all.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_acc = accuracy_score(labels_all, preds)\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # ---------- VALIDATION ----------\n",
        "    model.eval()\n",
        "    val_loss, val_preds, val_labels = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for premise, hypothesis_in, labels in val_loader:\n",
        "            premise, hypothesis_in, labels = premise.to(device), hypothesis_in.to(device), labels.to(device)\n",
        "            outputs = model(premise, hypothesis_in)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")"
      ],
      "metadata": {
        "id": "7FBxqJVgEjVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.4 Evaluation"
      ],
      "metadata": {
        "id": "7nr9KNsCIbpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(val_labels, val_preds, target_names=list(label_mapping.keys())))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(val_labels, val_preds)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(label_mapping.keys())).plot(cmap='Blues')\n",
        "plt.title(\"Validation Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss Curves\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
        "plt.plot(val_losses, label=\"Val Loss\", marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xqOzcdGUEmhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4w_n2P2_xxm"
      },
      "source": [
        "## 4.4 Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpZNRYhe_1ae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GU_rQZJk87q"
      },
      "source": [
        "# 5. Performance Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L11vnP-lk_ZH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvj4_3Qak_x7"
      },
      "source": [
        "# 6. Interactive Inference Colab Form\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMJN1Dx1lES9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}