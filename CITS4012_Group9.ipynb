{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOsq3No1kRC-"
   },
   "source": [
    "# 2025 CITS4012 Group 9 Assignment\n",
    "\n",
    "**Collaborators**\n",
    "\n",
    "| Uni ID   | Student Name  | GitHub Username                                   |\n",
    "| -------- | ------------- | ------------------------------------------------- |\n",
    "| 24141207 | Kaichao Zheng | [Kaichao-Zheng](https://github.com/Kaichao-Zheng) |\n",
    "| 24645175 | Ziqi Meng     | [jiongge39](https://github.com/jiongge39)         |\n",
    "| 23998001 | Yanglei Yuan  | [LeoYuan0225](https://github.com/LeoYuan0225)     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THS1ppSwkgz4"
   },
   "source": [
    "# Readme\n",
    "这是Project 1的模板，我直接拿来用了\n",
    "\n",
    "仅需注重代码风格及可视化，让marker改起来舒服就行，分数低不了的\n",
    "* 例如：多抄Lab的技术栈，多引用学界的基石论文，marker自然乐意审自己熟悉的技术。e.g., WordCloud Visualization\n",
    "\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object *Oriented* Programming style, please put those the bottom of this ipynb file*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LPgCx5t-0f8"
   },
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTV2-AYuV_lx"
   },
   "source": [
    "> NOTE:\n",
    ">\n",
    "> In Google Colab, an ERROR would occur due to incompatibility of the latest versions of `numpy` and `scipy`.\n",
    ">\n",
    "> Simply **restart the runtime** to use the newly downgraded versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XsKNGCKj_DxD",
    "outputId": "10878bbb-6387-461c-dcef-035817076c9d"
   },
   "outputs": [],
   "source": [
    "%pip install word2number\n",
    "%pip install contractions\n",
    "%pip install nltk\n",
    "%pip install pandas\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0at-kJs67gh"
   },
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcBQf2U37lwG"
   },
   "source": [
    "We implemented three substantially different model architectures:\n",
    "\n",
    "* [The vanilla RNN-based encoder-decoder](#scrollTo=67TQJgOJ_lF1)\n",
    "\n",
    "* [The Bi-LSTM encoder-decoder with XXX-attention in different positions](#scrollTo=BhSE5ON4_r0C)\n",
    "\n",
    "* [The vanilla Transformer with self-attention (no RNN)](#scrollTo=E4w_n2P2_xxm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tgj4JarkjAY"
   },
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eylhXsB4Fb_V"
   },
   "source": [
    "## 2.1 Load JSON files from GitHub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZlBU0GAUiTD",
    "outputId": "24f98f0e-ed27-4cf9-9443-a5c4e3d72432"
   },
   "outputs": [],
   "source": [
    "# Define JSON dataset paths\n",
    "base_url = \"https://raw.githubusercontent.com/Kaichao-Zheng/CITS4012-NLP/main/\"\n",
    "\n",
    "train_file = base_url + \"train.json\"\n",
    "val_file = base_url + \"validation.json\"\n",
    "test_file = base_url + \"test.json\"\n",
    "\n",
    "# Quick check\n",
    "print(\"✅ Dataset URLs:\")\n",
    "print(\"Train:\\t\", train_file)\n",
    "print(\"Val:\\t\", val_file)\n",
    "print(\"Test:\\t\", test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vF0lwUqfkj6p",
    "outputId": "a4fef407-541a-4c0b-e691-0f564710333f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Working dataframes\n",
    "train_df = pd.read_json(train_file)\n",
    "val_df = pd.read_json(val_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "\n",
    "# Keep original copies\n",
    "source_train_df = train_df.copy()\n",
    "source_val_df = val_df.copy()\n",
    "source_test_df = test_df.copy()\n",
    "\n",
    "# Sneak peek\n",
    "pd.set_option(\"display.max_colwidth\", 30)\n",
    "\n",
    "print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "pd.reset_option(\"display.max_colwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJZtrMRPnYld"
   },
   "source": [
    "## 2.2 Define difference viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7PZFNNUncY7"
   },
   "outputs": [],
   "source": [
    "import html\n",
    "from difflib import SequenceMatcher\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def verify_print(source_df, cleaned_df, tag, num, col, idx_src ,idx_now, kept=False):\n",
    "  status = \"Kept \" if kept else \"Reindexed\"\n",
    "  print(f\"{tag} {num}:\\tidx_src: {idx_src}\\t{source_df[col][idx_src]}\")\n",
    "  print(f\"{status} {num}:\\tidx_now: {idx_now}\\t{cleaned_df[col][idx_now]}\")\n",
    "  print()\n",
    "\n",
    "# You can enable dark mode by replacing default \"light\" here\n",
    "def verify_diff(source_df, cleaned_df, tag, num, col, index, kept=False, theme=\"dark\"):\n",
    "  if theme == \"light\":\n",
    "    bg = \"#fafafa\"; fg = \"#000000\"\n",
    "    del_bg = \"#ffdddd\"; ins_bg = \"#ddffdd\"\n",
    "    del_fg = \"#aa0000\"; ins_fg = \"#006600\"\n",
    "  else:\n",
    "    bg = \"#1e1e1e\"; fg = \"#e0e0e0\"\n",
    "    del_bg = \"#662222\"; ins_bg = \"#224422\"\n",
    "    del_fg = \"#ff9999\"; ins_fg = \"#99ff99\"\n",
    "\n",
    "  status = \"Kept\" if kept else \"Fixed\"\n",
    "\n",
    "  original = source_df[col][index]\n",
    "  cleaned = cleaned_df[col][index]\n",
    "\n",
    "  matcher = SequenceMatcher(None, original, cleaned)\n",
    "\n",
    "  html_original = f\"\"\"\n",
    "  <div style='margin-bottom: 10px; background-color:{bg}; color:{fg}; white-space: nowrap; font-family: monospace; padding: 5px; border-radius:4px;'>\n",
    "  <b>{tag} {num}:</b> \"\"\"\n",
    "  html_cleaned = f\"\"\"\n",
    "  <div style='margin-bottom: 10px; background-color:{bg}; color:{fg}; white-space: nowrap; font-family: monospace; padding: 5px; border-radius:4px;'>\n",
    "  <b>{status} {num}:</b> \"\"\"\n",
    "\n",
    "  for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "    orig_text = original[i1:i2].replace(\" \", \"&nbsp;\")\n",
    "    clean_text = cleaned[j1:j2].replace(\" \", \"&nbsp;\")\n",
    "\n",
    "    if tag == 'equal':\n",
    "      html_original += orig_text\n",
    "      html_cleaned += clean_text\n",
    "    elif tag == 'delete':\n",
    "      html_original += f\"<span style='background-color:{del_bg}; color:{del_fg};'>{orig_text}</span>\"\n",
    "    elif tag == 'insert':\n",
    "      html_cleaned += f\"<span style='background-color:{ins_bg}; color:{ins_fg};'>{clean_text}</span>\"\n",
    "    elif tag == 'replace':\n",
    "      html_original += f\"<span style='background-color:{del_bg}; color:{del_fg};'>{orig_text}</span>\"\n",
    "      html_cleaned += f\"<span style='background-color:{ins_bg}; color:{ins_fg};'>{clean_text}</span>\"\n",
    "\n",
    "  html_original += \"</div>\"\n",
    "  html_cleaned += \"</div>\"\n",
    "\n",
    "  display(HTML(html_original + html_cleaned + \"<hr/>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9ibjBESaH08"
   },
   "source": [
    "## 2.3 Data cleansing\n",
    "\n",
    "原则上，我们只知道训练集数据，不该为验证集/测试集的特点设计数据清洗的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzfrlfEJgpZ_"
   },
   "source": [
    "Assumption\n",
    "* The value of `label` is binary, either \"neutral\" or \"entails\".\n",
    "\n",
    "Compromises\n",
    "* Ignore syntactic errors and semantic errors.\n",
    "* Apply unified premises rules on hypothesis.\n",
    "\n",
    "Handled Issues\n",
    "| No. | Description | Examples |\n",
    "|----------|-------------|-----------|\n",
    "| Issue 1 | HTML/XML tags with ID pattern | train premise 78, 270, 319, ... |\n",
    "| Issue 2 | Non-linguistic long/pure separators | train premise 1, 382, 385, ... |\n",
    "| Issue 3 | Duplicate consecutive phrases | train premise 78, 87, 564, ... |\n",
    "| Issue 4 | Single-word sentences  | train premise 146, 181, 427, ... |\n",
    "| Issue 5 | Duplicated whitespaces | train premise 123, 193, 259, ... |\n",
    "| Issue 6 | Spaces before punctuations, except '!' and '?' | train premise 3, 333, 6280 |\n",
    "| Issue 7 | Premise with long concatenated sentences | train premise 270, 537, 608, ... |\n",
    "\n",
    "Kept Noises\n",
    "\n",
    "| No. | Description | Noise | Non-Noise |\n",
    "|----------|-------------|----------|--------------|\n",
    "| Noise 1 | Instructional prompt words | train premise 3, 61, 319, ... | train premise 16, 24, 61, ... |\n",
    "| Noise 2 | Numbered markers | train premise 270, 537, 608, ... | train premise 1546, 2068, ... |\n",
    "| Noise 3 | Misplaced `label` values | train premise 270, 537, 606, ... | train premise 1683, 2068, ... |\n",
    "| Noise 4 | Metadata prefixes | train premise 32, 230, 482, ... |  |\n",
    "| Noise 4 | Isolated single symbols | train premise 60, 1185 | comparison operators |\n",
    "\n",
    "\n",
    "Limitation\n",
    "\n",
    "The difference viewer automatically escapes HTML entities, which might overlook HTML noise. (E.g., &amp;quot; in train premise 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "w1jUqkRJadr_",
    "outputId": "5ba3dc94-8039-40f9-fcce-0db60cc937db"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanse(df):\n",
    "  df = df.copy()\n",
    "\n",
    "  ID_PATTERN = r\"\\b[A-Za-z]?(?:\\d{6,}|[A-Za-z0-9]{8,})(?:-[A-Za-z0-9]{2,})+\\b\"\n",
    "  REPEAT_PATTERN = r\"\\b((?:\\w+\\s+){0,2}\\w+)( \\1\\b)+\"\n",
    "\n",
    "  for col in [\"premise\", \"hypothesis\"]:\n",
    "\n",
    "    # pre-trim: two-tailed whitespaces\n",
    "    df[col] = df[col].apply(lambda x: x.strip())\n",
    "\n",
    "    # issue 1: HTML/XML tags with ID pattern\n",
    "    df[col] = df[col].apply(html.unescape)\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"<[^>]*>\", \" \", x))\n",
    "    df[col] = df[col].apply(lambda x: re.sub(ID_PATTERN, \" \", x))\n",
    "\n",
    "    # issue 2: non-linguistic long/pure separators\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"[-=*_~$]{3,}\", \" \", x))\n",
    "    df[col] = df[col].apply(lambda x: \"\" if re.fullmatch(r\"[\\W_]+\", x.strip()) else x)\n",
    "\n",
    "    # issue 3: duplicate consecutive phrases\n",
    "    df[col] = df[col].apply(lambda x: re.sub(REPEAT_PATTERN, r\"\\1\", x))\n",
    "\n",
    "    # issue 4: single-word sentences\n",
    "    df[col] = df[col].apply(lambda x: \"\" if re.fullmatch(r\"(\\w+[.!?']?|[^\\w\\s]+)\", x.strip()) else x)\n",
    "\n",
    "    # issue 5: duplicate whitespaces\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
    "\n",
    "    # issue 6: spaces before punctuations\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"\\s+([.,;:])\", r\"\\1\", x))\n",
    "\n",
    "  return df\n",
    "\n",
    "# Clenasing\n",
    "train_df = cleanse(train_df)\n",
    "val_df = cleanse(val_df)\n",
    "test_df = cleanse(test_df)\n",
    "\n",
    "# Keep cleaned copies\n",
    "cleaned_train_df = train_df.copy()\n",
    "cleaned_val_df = val_df.copy()\n",
    "cleaned_test_df = test_df.copy()\n",
    "\n",
    "# Verification\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=1, col=\"premise\", index=319)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=2, col=\"premise\", index=1)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=3, col=\"premise\", index=87)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=4, col=\"premise\", index=146)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=5, col=\"premise\", index=123)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=6, col=\"premise\", index=3)\n",
    "# verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=6, col=\"premise\", index=6280, kept=True)  # why except '?'\n",
    "# verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=6, col=\"premise\", index=333, kept=True)  # why except '!'\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=\"Hybrid\", col=\"premise\", index=270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDOTLdmDKwfy"
   },
   "source": [
    "## 2.4 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rc0V-G-Ei8lr",
    "outputId": "1c3774a6-d916-490f-ae3f-412acf63feaa"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "sww = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "gAEbhAKoKyEW",
    "outputId": "7c198f46-cb94-40e2-850f-3137ba8940ad"
   },
   "outputs": [],
   "source": [
    "import re, contractions\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from word2number import w2n\n",
    "from contractions import fix as expand_contractions\n",
    "\n",
    "def word2num(text):\n",
    "  def convert(match):\n",
    "    word = match.group(0)\n",
    "    try:\n",
    "      return f\" {w2n.word_to_num(word)} \"\n",
    "    except:\n",
    "      return f\" {word} \"\n",
    "  text = re.sub(\n",
    "    r'\\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten|'\n",
    "    r'eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|'\n",
    "    r'eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|'\n",
    "    r'eighty|ninety|hundred|thousand|million|billion|and|[- ])+\\b',\n",
    "    convert, text)\n",
    "  return ' '.join(text.split())\n",
    "\n",
    "def normalize(df):\n",
    "  df = df.copy()\n",
    "\n",
    "  for col in [\"premise\", \"hypothesis\"]:\n",
    "\n",
    "    # rule 1: lowercase\n",
    "    df[col] = df[col].str.lower()\n",
    "\n",
    "    # rule 2: expand contraction\n",
    "    df[col] = df[col].apply(expand_contractions)\n",
    "\n",
    "    # rule -1: remove stopwords\n",
    "    # df[col] = df[col].apply(lambda x: \" \".join([w for w in word_tokenize(x) if w.lower() not in sww]))\n",
    "\n",
    "    # rule -2: remove bracketed content\n",
    "    # df[col] = df[col].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x))\n",
    "\n",
    "    # rule -3: symbolize linguistic numbers\n",
    "    # df[col] = df[col].apply(word2num)\n",
    "\n",
    "  return df\n",
    "\n",
    "# Normalization\n",
    "train_df = normalize(train_df)\n",
    "val_df = normalize(val_df)\n",
    "test_df = normalize(test_df)\n",
    "\n",
    "# Keep normalized copies\n",
    "normalized_train_df = train_df.copy()\n",
    "normalized_val_df = val_df.copy()\n",
    "normalized_test_df = test_df.copy()\n",
    "\n",
    "# Verification\n",
    "verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=\"1\", col=\"premise\", index=1)\n",
    "verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=\"2\", col=\"premise\", index=420)\n",
    "verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=\"2\", col=\"hypothesis\", index=8, kept=True)\n",
    "\n",
    "# Inactivate rules\n",
    "# verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=-1, col=\"premise\", index=1, kept=True)\n",
    "# verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=-2, col=\"premise\", index=8, kept=True)\n",
    "# verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=-3, col=\"premise\", index=147, kept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1j8PKb8i3zP"
   },
   "source": [
    "## 2.5 Reindexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzpo8vldi509",
    "outputId": "064ea0c2-1412-47eb-8724-7d85c8fba3d6"
   },
   "outputs": [],
   "source": [
    "def reindex(df):\n",
    "  df = df.copy()\n",
    "\n",
    "  for col in [\"premise\", \"hypothesis\"]:\n",
    "\n",
    "    # post-issue 4: remove rows with empty cell\n",
    "    df = df[df[col].astype(str).str.strip() != \"\"]\n",
    "\n",
    "  # issue 7: premise with long concatenated sentences\n",
    "  new_rows = []\n",
    "  for _, row in df.iterrows():\n",
    "    premise = str(row[\"premise\"]).strip()\n",
    "    hypothesis = str(row[\"hypothesis\"]).strip()\n",
    "    label = str(row[\"label\"]).strip()\n",
    "\n",
    "    if not premise:\n",
    "      continue\n",
    "\n",
    "    # split by \". \" or \"; \" to avoid 3.14\n",
    "    sentences = re.split(r'(?<!\\d)(?<=[.;])\\s+(?=[A-Za-z])', premise)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    for s in sentences:\n",
    "      new_rows.append({\n",
    "        \"premise\": s,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "      })\n",
    "  df = pd.DataFrame(new_rows)\n",
    "\n",
    "  # reorder index\n",
    "  df = df.reset_index(drop=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "# Reindexing\n",
    "train_df = reindex(train_df)\n",
    "val_df = reindex(val_df)\n",
    "test_df = reindex(test_df)\n",
    "\n",
    "# Keep reindexed copies\n",
    "reindexed_train_df = train_df.copy()\n",
    "reindexed_val_df = val_df.copy()\n",
    "reindexed_test_df = test_df.copy()\n",
    "\n",
    "# Verification\n",
    "verify_print(normalized_train_df, reindexed_train_df, tag=\"Post-Issue\", num=4, col=\"premise\", idx_src=147, idx_now=158)\n",
    "verify_print(normalized_train_df, reindexed_train_df, tag=\"Issue\", num=7, col=\"premise\", idx_src=31, idx_now=31, kept=True)\n",
    "verify_print(normalized_train_df, reindexed_train_df, tag=\"Issue\", num=7, col=\"premise\", idx_src=270, idx_now=281)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adtX9t6XKyTd"
   },
   "source": [
    "## 2.6 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mX8ghj_AK0uG",
    "outputId": "0f7c1825-37d6-4eeb-a786-4d579e875827"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "ZVPHolC0DPzA",
    "outputId": "71134401-ba23-4459-92eb-82b4f2159f7e"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def protect_float(text: str) -> str:\n",
    "  return re.sub(r\"(?<=\\d)\\.(?=\\d)\", \"DOTTK\", text)\n",
    "\n",
    "def recover_float(tokens):\n",
    "  return [t.replace(\"DOTTK\", \".\") for t in tokens]\n",
    "\n",
    "def tokenize(df):\n",
    "  df = df.copy()\n",
    "  KEEP_SYMBOLS = set(\"=<>+-/*%!\")\n",
    "\n",
    "  for col in [\"premise\", \"hypothesis\"]:\n",
    "    # Remove punctuations\n",
    "    df[col] = df[col].apply(lambda x: protect_float(x))\n",
    "    df[col] = df[col].apply(lambda x: [t for t in word_tokenize(x) if t not in string.punctuation or t in KEEP_SYMBOLS])\n",
    "    df[col] = df[col].apply(lambda toks: recover_float(toks))\n",
    "\n",
    "  return df\n",
    "\n",
    "# Tokenization\n",
    "tokenized_train_df = tokenize(train_df)\n",
    "tokenized_val_df = tokenize(val_df)\n",
    "tokenized_test_df = tokenize(test_df)\n",
    "\n",
    "# Quick check\n",
    "preview_rows = pd.concat([\n",
    "  tokenized_train_df.head(),\n",
    "  tokenized_train_df.iloc[[6, 23, 34, 62, 369, 438, 2116]]  # concatenate indeces 6, 23, 32, 60, 319, 385, 1185\n",
    "])\n",
    "\n",
    "display(preview_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm9wewk0QTzH"
   },
   "source": [
    "## 2.7 Visualisation via Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b48-JUWxRG28"
   },
   "source": [
    "We actually include stopwords in the tokenized dataframes for model training,\n",
    "\n",
    "but temporarily remove them here for visualisation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "0robldKbRWOK",
    "outputId": "7f51077c-031b-4009-8bd1-9eeb14301dd7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "sww = stopwords.words('english')\n",
    "\n",
    "for col in [\"premise\", \"hypothesis\"]:\n",
    "  tokens = []\n",
    "  # Remomve stopwords temporarily\n",
    "  for toks in tokenized_train_df[col]:\n",
    "    tokens.extend([t.lower() for t in toks if t.isalpha() and t.lower() not in sww])\n",
    "  wordcloud = WordCloud(background_color=\"white\").generate(\" \".join(tokens))\n",
    "  plt.figure()\n",
    "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "  plt.axis(\"off\")\n",
    "  plt.title(f\"{col.capitalize()} Word Cloud (Train Set)\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDxZSK20klWr"
   },
   "source": [
    "# 3. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ztqzVInTC1d"
   },
   "source": [
    "## 3.1 Signal tags\n",
    "\n",
    "We chose to add \\<EOS> after premise to signal the end of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fleoeIBDkr5i",
    "outputId": "3991beed-6fbb-462e-ce0a-f9c5980986e7"
   },
   "outputs": [],
   "source": [
    "def tag_seq2seq(df):\n",
    "  df = df.copy()\n",
    "\n",
    "  # Encoder input sequences\n",
    "  df[\"premise\"] = df[\"premise\"].apply(lambda s: s + [\"<EOS>\"])\n",
    "\n",
    "  # Decoder input sequences\n",
    "  df[\"hypothesis_in\"] = df[\"hypothesis\"].apply(lambda s: [\"<BOS>\"] + s)\n",
    "\n",
    "  # Decoder target sequences for teacher forcing\n",
    "  df[\"hypothesis_tar\"] = df[\"hypothesis\"].apply(lambda s: s + [\"<EOS>\"])\n",
    "\n",
    "  return df\n",
    "\n",
    "# Reorder columns\n",
    "order = [\"premise\",\"hypothesis_in\",\"hypothesis_tar\",\"label\"]\n",
    "\n",
    "train_seq2seq_df = tag_seq2seq(tokenized_train_df)[order]\n",
    "val_seq2seq_df = tag_seq2seq(tokenized_val_df)[order]\n",
    "test_seq2seq_df = tag_seq2seq(tokenized_test_df)[order]\n",
    "\n",
    "display(train_seq2seq_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoSbUcLGaURF"
   },
   "source": [
    "## 3.2 Token vocabulary\n",
    "\n",
    "The vocabulary only includes tokens with `frequency >= 3`.\n",
    "\n",
    "Therefore, a token with low frequency will be regarded as \\<UNK>.\n",
    "\n",
    "E.g. the token \"6.39\" from the first premise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6MnwSgIVx58",
    "outputId": "19a01808-8e92-42f4-b3a1-9854a1062edc"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(tagged_df, min_freq=3):\n",
    "  counter = Counter()\n",
    "  for col in [\"premise\", \"hypothesis\"]:\n",
    "    for toks in tagged_df[col]:\n",
    "      counter.update(toks)\n",
    "\n",
    "  # special tokens\n",
    "  vocab = {\"<PAD>\": 0,\"<UNK>\": 1,\"<BOS>\": 2,\"<EOS>\": 3}\n",
    "  for token, freq in counter.items():\n",
    "    if freq >= min_freq and token not in vocab:\n",
    "      vocab[token] = len(vocab)\n",
    "\n",
    "  return vocab\n",
    "\n",
    "vocab = build_vocab(tokenized_train_df)\n",
    "print(\"Vocab size:\", {len(vocab)})\n",
    "print(\"Sample tokens:\", list(vocab.keys())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVw81ZCbgjSh"
   },
   "source": [
    "## 3.3 One-hot key embedding\n",
    "\n",
    "Map each token to its one-hot key from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gg2wWyg-bV3r",
    "outputId": "2a90d6fc-6fcb-49a2-ad98-7cee7dc9c9f6"
   },
   "outputs": [],
   "source": [
    "def word_to_index(df, vocab):\n",
    "  df = df.copy()\n",
    "\n",
    "  for col in [\"premise\", \"hypothesis_in\", \"hypothesis_tar\"]:\n",
    "    df[col] = df[col].apply(lambda toks: [vocab.get(w, vocab[\"<UNK>\"]) for w in toks])\n",
    "\n",
    "  label_map = {\"neutral\": 0, \"entails\": 1}\n",
    "  if \"label\" in df.columns:\n",
    "    df[\"label\"] = df[\"label\"].map(label_map).fillna(-1).astype(int)\n",
    "\n",
    "  return df\n",
    "\n",
    "indexed_train_df = word_to_index(train_seq2seq_df, vocab)\n",
    "\n",
    "display(indexed_train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZJlo-z8k0ai"
   },
   "source": [
    "# 4. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyhnsNdv7kVj"
   },
   "source": [
    "### Shared hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgNqexCv7fHf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67TQJgOJ_lF1"
   },
   "source": [
    "## 4.1 The vanilla RNN-based encoder-decoder\n",
    "\n",
    "Lab 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRXewgTnk6sB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhSE5ON4_r0C"
   },
   "source": [
    "## 4.2 The Bi-LSTM encoder-decoder with XXX-attention in different positions\n",
    "\n",
    "Lab 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b14s1AoN_wNC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4w_n2P2_xxm"
   },
   "source": [
    "## 4.3 The vanilla Transformer with self-attention\n",
    "\n",
    "Lab 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpZNRYhe_1ae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GU_rQZJk87q"
   },
   "source": [
    "# 5. Performance Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L11vnP-lk_ZH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zvj4_3Qak_x7"
   },
   "source": [
    "# 6. Interactive Inference Colab Form\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMJN1Dx1lES9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
