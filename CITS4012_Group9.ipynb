{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOsq3No1kRC-"
   },
   "source": [
    "# 2025 CITS4012 Group 9 Assignment\n",
    "\n",
    "**Collaborators**\n",
    "\n",
    "| Uni ID   | Student Name  | GitHub Username                                   |\n",
    "| -------- | ------------- | ------------------------------------------------- |\n",
    "| 24141207 | Kaichao Zheng | [Kaichao-Zheng](https://github.com/Kaichao-Zheng) |\n",
    "| 24645175 | Ziqi Meng     | [jiongge39](https://github.com/jiongge39)         |\n",
    "| 23998001 | Yanglei Yuan  | [LeoYuan0225](https://github.com/LeoYuan0225)     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THS1ppSwkgz4"
   },
   "source": [
    "# Readme\n",
    "这是Project 1的模板，我直接拿来用了\n",
    "\n",
    "仅需注重代码风格及可视化，让marker改起来舒服就行，分数低不了的\n",
    "* 例如：多抄Lab的技术栈，多引用学界的基石论文，marker自然乐意审自己熟悉的技术。e.g., WordCloud Visualization\n",
    "\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object *Oriented* Programming style, please put those the bottom of this ipynb file*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LPgCx5t-0f8"
   },
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTV2-AYuV_lx"
   },
   "source": [
    "> NOTE:\n",
    ">\n",
    "> In Google Colab, an ERROR would occur due to incompatibility of the latest versions of `numpy` and `scipy`.\n",
    ">\n",
    "> Simply **restart the runtime** to use the newly downgraded versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XsKNGCKj_DxD",
    "outputId": "2a825ef3-3aa2-4a3f-9aee-e9fb1dc906fb"
   },
   "outputs": [],
   "source": [
    "%pip install word2number\n",
    "%pip install contractions\n",
    "%pip install nltk\n",
    "%pip install pandas\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0at-kJs67gh"
   },
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcBQf2U37lwG"
   },
   "source": [
    "We implemented three substantially different model architectures:\n",
    "\n",
    "* [The vanilla RNN encoder-decoder architecture](#scrollTo=67TQJgOJ_lF1)\n",
    "\n",
    "* [The Bi-LSTM encoder-decoder with XXX-attention in different positions](#scrollTo=BhSE5ON4_r0C)\n",
    "\n",
    "* [The vanilla Transformer with self-attention (no RNN)](#scrollTo=E4w_n2P2_xxm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tgj4JarkjAY"
   },
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eylhXsB4Fb_V"
   },
   "source": [
    "## 2.1 Load JSON files from GitHub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZlBU0GAUiTD",
    "outputId": "d189400f-cb85-4d03-b5f1-427b95753e3d"
   },
   "outputs": [],
   "source": [
    "# Define JSON dataset paths\n",
    "base_url = \"https://raw.githubusercontent.com/Kaichao-Zheng/CITS4012-NLP/main/\"\n",
    "\n",
    "train_file = base_url + \"train.json\"\n",
    "val_file = base_url + \"validation.json\"\n",
    "test_file = base_url + \"test.json\"\n",
    "\n",
    "# Quick check\n",
    "print(\"✅ Dataset URLs:\")\n",
    "print(\"Train:\\t\", train_file)\n",
    "print(\"Val:\\t\", val_file)\n",
    "print(\"Test:\\t\", test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vF0lwUqfkj6p",
    "outputId": "fe93b844-9951-4bdc-e233-8ab9c4b16717"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Working dataframes\n",
    "train_df = pd.read_json(train_file)\n",
    "val_df = pd.read_json(val_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "\n",
    "# Keep original copies\n",
    "source_train_df = train_df.copy()\n",
    "source_val_df = val_df.copy()\n",
    "source_test_df = test_df.copy()\n",
    "\n",
    "# Sneak peek\n",
    "pd.set_option(\"display.max_colwidth\", 30)\n",
    "\n",
    "print(train_df.head())\n",
    "# print(val_df.head())\n",
    "# print(test_df.head())\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "pd.reset_option(\"display.max_colwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJZtrMRPnYld"
   },
   "source": [
    "## 2.2 Define difference viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7PZFNNUncY7"
   },
   "outputs": [],
   "source": [
    "import html\n",
    "from difflib import SequenceMatcher\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def verify_print(source_df, cleaned_df, tag, num, col, idx_src ,idx_now, kept=False):\n",
    "  status = \"Kept \" if kept else \"Reindexed\"\n",
    "  print(f\"{tag} {num}:\\tidx_src: {idx_src}\\t{source_df[col][idx_src]}\")\n",
    "  print(f\"{status} {num}:\\tidx_now: {idx_now}\\t{cleaned_df[col][idx_now]}\")\n",
    "  print()\n",
    "\n",
    "# You can enable dark mode by replacing default \"light\" here\n",
    "def verify_diff(source_df, cleaned_df, tag, num, col, index, kept=False, theme=\"dark\"):\n",
    "  if theme == \"light\":\n",
    "    bg = \"#fafafa\"; fg = \"#000000\"\n",
    "    del_bg = \"#ffdddd\"; ins_bg = \"#ddffdd\"\n",
    "    del_fg = \"#aa0000\"; ins_fg = \"#006600\"\n",
    "  else:\n",
    "    bg = \"#1e1e1e\"; fg = \"#e0e0e0\"\n",
    "    del_bg = \"#662222\"; ins_bg = \"#224422\"\n",
    "    del_fg = \"#ff9999\"; ins_fg = \"#99ff99\"\n",
    "\n",
    "  status = \"Kept\" if kept else \"Fixed\"\n",
    "\n",
    "  original = source_df[col][index]\n",
    "  cleaned = cleaned_df[col][index]\n",
    "\n",
    "  matcher = SequenceMatcher(None, original, cleaned)\n",
    "\n",
    "  html_original = f\"\"\"\n",
    "  <div style='margin-bottom: 10px; background-color:{bg}; color:{fg}; white-space: nowrap; font-family: monospace; padding: 5px; border-radius:4px;'>\n",
    "  <b>{tag} {num}:</b> \"\"\"\n",
    "  html_cleaned = f\"\"\"\n",
    "  <div style='margin-bottom: 10px; background-color:{bg}; color:{fg}; white-space: nowrap; font-family: monospace; padding: 5px; border-radius:4px;'>\n",
    "  <b>{status} {num}:</b> \"\"\"\n",
    "\n",
    "  for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "    orig_text = original[i1:i2].replace(\" \", \"&nbsp;\")\n",
    "    clean_text = cleaned[j1:j2].replace(\" \", \"&nbsp;\")\n",
    "\n",
    "    if tag == 'equal':\n",
    "      html_original += orig_text\n",
    "      html_cleaned += clean_text\n",
    "    elif tag == 'delete':\n",
    "      html_original += f\"<span style='background-color:{del_bg}; color:{del_fg};'>{orig_text}</span>\"\n",
    "    elif tag == 'insert':\n",
    "      html_cleaned += f\"<span style='background-color:{ins_bg}; color:{ins_fg};'>{clean_text}</span>\"\n",
    "    elif tag == 'replace':\n",
    "      html_original += f\"<span style='background-color:{del_bg}; color:{del_fg};'>{orig_text}</span>\"\n",
    "      html_cleaned += f\"<span style='background-color:{ins_bg}; color:{ins_fg};'>{clean_text}</span>\"\n",
    "\n",
    "  html_original += \"</div>\"\n",
    "  html_cleaned += \"</div>\"\n",
    "\n",
    "  display(HTML(html_original + html_cleaned + \"<hr/>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9ibjBESaH08"
   },
   "source": [
    "## 2.3 Data cleansing\n",
    "\n",
    "原则上，我们只知道训练集数据，不该为验证集/测试集的特点设计数据清洗的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzfrlfEJgpZ_"
   },
   "source": [
    "Assumption\n",
    "* The value of `label` is binary, either \"neutral\" or \"entails\".\n",
    "\n",
    "Compromises\n",
    "* Ignore syntactic errors and semantic errors.\n",
    "* Apply unified premises rules on hypothesis.\n",
    "\n",
    "Handled Issues\n",
    "| No. | Description | Examples |\n",
    "|----------|-------------|-----------|\n",
    "| Issue 1 | HTML/XML tags with ID pattern | train premise 78, 270, 319, ... |\n",
    "| Issue 2 | Non-linguistic long/pure separators | train premise 1, 382, 385, ... |\n",
    "| Issue 3 | Duplicate consecutive phrases | train premise 78, 87, 564, ... |\n",
    "| Issue 4 | Single-word sentences  | train premise 146, 181, 427, ... |\n",
    "| Issue 5 | Duplicated whitespaces | train premise 123, 193, 259, ... |\n",
    "| Issue 6 | Spaces before punctuations, except '!' and '?' | train premise 3, 333, 6280 |\n",
    "| Issue 7 | Premise with long concatenated sentences | train premise 270, 537, 608, ... |\n",
    "\n",
    "Kept Noises\n",
    "\n",
    "| No. | Description | Noise | Non-Noise |\n",
    "|----------|-------------|----------|--------------|\n",
    "| Noise 1 | Instructional prompt words | train premise 3, 61, 319, ... | train premise 16, 24, 61, ... |\n",
    "| Noise 2 | Numbered markers | train premise 270, 537, 608, ... | train premise 1546, 2068, ... |\n",
    "| Noise 3 | Misplaced `label` values | train premise 270, 537, 606, ... | train premise 1683, 2068, ... |\n",
    "| Noise 4 | Metadata prefixes | train premise 32, 230, 482, ... |  |\n",
    "| Noise 4 | Isolated single symbols | train premise 60, 1185 | comparison operators |\n",
    "\n",
    "\n",
    "Limitation\n",
    "\n",
    "The difference viewer automatically escapes HTML entities, which might overlook HTML noise. (E.g., &amp;quot; in train premise 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "w1jUqkRJadr_",
    "outputId": "537224e6-c2dd-43f0-9f82-83d625d9a689"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanse(df):\n",
    "  df = df.copy()\n",
    "\n",
    "  ID_PATTERN = r\"\\b[A-Za-z]?(?:\\d{6,}|[A-Za-z0-9]{8,})(?:-[A-Za-z0-9]{2,})+\\b\"\n",
    "  REPEAT_PATTERN = r\"\\b((?:\\w+\\s+){0,2}\\w+)( \\1\\b)+\"\n",
    "\n",
    "  for col in [\"premise\", \"hypothesis\"]:\n",
    "\n",
    "    # pre-trim: two-tailed whitespaces\n",
    "    df[col] = df[col].apply(lambda x: x.strip())\n",
    "\n",
    "    # issue 1: HTML/XML tags with ID pattern\n",
    "    df[col] = df[col].apply(html.unescape)\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"<[^>]*>\", \" \", x))\n",
    "    df[col] = df[col].apply(lambda x: re.sub(ID_PATTERN, \" \", x))\n",
    "\n",
    "    # issue 2: non-linguistic long/pure separators\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"[-=*_~$]{3,}\", \" \", x))\n",
    "    df[col] = df[col].apply(lambda x: \"\" if re.fullmatch(r\"[\\W_]+\", x.strip()) else x)\n",
    "\n",
    "    # issue 3: duplicate consecutive phrases\n",
    "    df[col] = df[col].apply(lambda x: re.sub(REPEAT_PATTERN, r\"\\1\", x))\n",
    "\n",
    "    # issue 4: single-word sentences\n",
    "    df[col] = df[col].apply(lambda x: \"\" if re.fullmatch(r\"(\\w+[.!?']?|[^\\w\\s]+)\", x.strip()) else x)\n",
    "\n",
    "    # issue 5: duplicate whitespaces\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())\n",
    "\n",
    "    # issue 6: spaces before punctuations\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r\"\\s+([.,;:])\", r\"\\1\", x))\n",
    "\n",
    "  return df\n",
    "\n",
    "# Clenasing\n",
    "train_df = cleanse(train_df)\n",
    "val_df = cleanse(val_df)\n",
    "test_df = cleanse(test_df)\n",
    "\n",
    "# Keep cleaned copies\n",
    "cleaned_train_df = train_df.copy()\n",
    "cleaned_val_df = val_df.copy()\n",
    "cleaned_test_df = test_df.copy()\n",
    "\n",
    "# Verification\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=1, col=\"premise\", index=319)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=2, col=\"premise\", index=1)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=3, col=\"premise\", index=87)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=4, col=\"premise\", index=146)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=5, col=\"premise\", index=123)\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=6, col=\"premise\", index=3)\n",
    "# verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=6, col=\"premise\", index=6280, kept=True)  # why except '?'\n",
    "# verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=6, col=\"premise\", index=333, kept=True)  # why except '!'\n",
    "verify_diff(source_train_df, cleaned_train_df, tag=\"Issue\", num=\"Hybrid\", col=\"premise\", index=270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDOTLdmDKwfy"
   },
   "source": [
    "## 2.4 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rc0V-G-Ei8lr",
    "outputId": "27f89fa2-6139-4618-b9cc-edb5ef44a311"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "sww = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "gAEbhAKoKyEW",
    "outputId": "ef523962-25f4-438e-e930-bceab4ca72da"
   },
   "outputs": [],
   "source": [
    "import re, contractions\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from word2number import w2n\n",
    "from contractions import fix as expand_contractions\n",
    "\n",
    "def word2num(text):\n",
    "  def convert(match):\n",
    "    word = match.group(0)\n",
    "    try:\n",
    "      return f\" {w2n.word_to_num(word)} \"\n",
    "    except:\n",
    "      return f\" {word} \"\n",
    "  text = re.sub(\n",
    "    r'\\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten|'\n",
    "    r'eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|'\n",
    "    r'eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|'\n",
    "    r'eighty|ninety|hundred|thousand|million|billion|and|[- ])+\\b',\n",
    "    convert, text)\n",
    "  return ' '.join(text.split())\n",
    "\n",
    "def normalize(df):\n",
    "  df = df.copy()\n",
    "\n",
    "  for col in [\"premise\", \"hypothesis\"]:\n",
    "\n",
    "    # rule 1: lowercase\n",
    "    df[col] = df[col].str.lower()\n",
    "\n",
    "    # rule 2: expand contraction\n",
    "    df[col] = df[col].apply(expand_contractions)\n",
    "\n",
    "    # rule -1: remove stopwords\n",
    "    # df[col] = df[col].apply(lambda x: \" \".join([w for w in word_tokenize(x) if w.lower() not in sww]))\n",
    "\n",
    "    # rule -2: remove bracketed content\n",
    "    # df[col] = df[col].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x))\n",
    "\n",
    "    # rule -3: symbolize linguistic numbers\n",
    "    # df[col] = df[col].apply(word2num)\n",
    "\n",
    "  return df\n",
    "\n",
    "# Normalization\n",
    "train_df = normalize(train_df)\n",
    "val_df = normalize(val_df)\n",
    "test_df = normalize(test_df)\n",
    "\n",
    "# Keep normalized copies\n",
    "normalized_train_df = train_df.copy()\n",
    "normalized_val_df = val_df.copy()\n",
    "normalized_test_df = test_df.copy()\n",
    "\n",
    "# Verification\n",
    "verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=\"1\", col=\"premise\", index=1)\n",
    "verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=\"2\", col=\"premise\", index=420)\n",
    "verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=\"2\", col=\"hypothesis\", index=8, kept=True)\n",
    "\n",
    "# Inactivate rules\n",
    "# verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=-1, col=\"premise\", index=1, kept=True)\n",
    "# verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=-2, col=\"premise\", index=8, kept=True)\n",
    "# verify_diff(cleaned_train_df, normalized_train_df, tag=\"Rule\", num=-3, col=\"premise\", index=147, kept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1j8PKb8i3zP"
   },
   "source": [
    "## 2.5 Reindexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzpo8vldi509",
    "outputId": "f15d5c16-8a6c-4551-91b4-85c44fa8c2cc"
   },
   "outputs": [],
   "source": [
    "def reindex(df):\n",
    "  df = df.copy()\n",
    "\n",
    "  for col in [\"premise\", \"hypothesis\"]:\n",
    "\n",
    "    # post-issue 4: remove rows with empty cell\n",
    "    df = df[df[col].astype(str).str.strip() != \"\"]\n",
    "\n",
    "  # issue 7: premise with long concatenated sentences\n",
    "  new_rows = []\n",
    "  for _, row in df.iterrows():\n",
    "    premise = str(row[\"premise\"]).strip()\n",
    "    hypothesis = str(row[\"hypothesis\"]).strip()\n",
    "    label = str(row[\"label\"]).strip()\n",
    "\n",
    "    if not premise:\n",
    "      continue\n",
    "\n",
    "    # split by \". \" or \"; \" to avoid 3.14\n",
    "    sentences = re.split(r'(?<!\\d)(?<=[.;])\\s+(?=[A-Za-z])', premise)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    for s in sentences:\n",
    "      new_rows.append({\n",
    "        \"premise\": s,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "      })\n",
    "  df = pd.DataFrame(new_rows)\n",
    "\n",
    "  # reorder index\n",
    "  df = df.reset_index(drop=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "# Reindexing\n",
    "train_df = reindex(train_df)\n",
    "val_df = reindex(val_df)\n",
    "test_df = reindex(test_df)\n",
    "\n",
    "# Keep reindexed copies\n",
    "reindexed_train_df = train_df.copy()\n",
    "reindexed_val_df = val_df.copy()\n",
    "reindexed_test_df = test_df.copy()\n",
    "\n",
    "# Verification\n",
    "verify_print(normalized_train_df, reindexed_train_df, tag=\"Post-Issue\", num=4, col=\"premise\", idx_src=147, idx_now=158)\n",
    "verify_print(normalized_train_df, reindexed_train_df, tag=\"Issue\", num=7, col=\"premise\", idx_src=31, idx_now=31, kept=True)\n",
    "verify_print(normalized_train_df, reindexed_train_df, tag=\"Issue\", num=7, col=\"premise\", idx_src=270, idx_now=281)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adtX9t6XKyTd"
   },
   "source": [
    "## 2.6 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mX8ghj_AK0uG",
    "outputId": "aba4e65a-9be5-4632-8fff-7453c84cfe47"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "ZVPHolC0DPzA",
    "outputId": "e17913b4-bb79-4bfb-ea1c-0263ff8f04f7"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def protect_float(text: str) -> str:\n",
    "  return re.sub(r\"(?<=\\d)\\.(?=\\d)\", \"DOTTK\", text)\n",
    "\n",
    "def recover_float(tokens):\n",
    "  return [t.replace(\"DOTTK\", \".\") for t in tokens]\n",
    "\n",
    "def tokenize(df):\n",
    "  df = df.copy()\n",
    "  KEEP_SYMBOLS = set(\"=<>+-/*%!\")\n",
    "\n",
    "  for col in [\"premise\", \"hypothesis\"]:\n",
    "    # Remove punctuations\n",
    "    df[col] = df[col].apply(lambda x: protect_float(x))\n",
    "    df[col] = df[col].apply(lambda x: [t for t in word_tokenize(x) if t not in string.punctuation or t in KEEP_SYMBOLS])\n",
    "    df[col] = df[col].apply(lambda toks: recover_float(toks))\n",
    "\n",
    "  return df\n",
    "\n",
    "# Tokenization\n",
    "tokenized_train_df = tokenize(train_df)\n",
    "tokenized_val_df = tokenize(val_df)\n",
    "tokenized_test_df = tokenize(test_df)\n",
    "\n",
    "# Quick check\n",
    "preview_rows = pd.concat([\n",
    "  tokenized_train_df.head(),\n",
    "  tokenized_train_df.iloc[[6, 23, 34, 62, 369, 438, 2116]]  # concatenate indeces 6, 23, 32, 60, 319, 385, 1185\n",
    "])\n",
    "\n",
    "display(preview_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm9wewk0QTzH"
   },
   "source": [
    "## 2.7 Visualisation via Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b48-JUWxRG28"
   },
   "source": [
    "We actually include stopwords in the tokenized dataframes for model training,\n",
    "\n",
    "but temporarily remove them here for visualisation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "0robldKbRWOK",
    "outputId": "4a78a19d-8a2a-45e0-e45e-0ff481652661"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "sww = stopwords.words('english')\n",
    "\n",
    "for col in [\"premise\", \"hypothesis\"]:\n",
    "  tokens = []\n",
    "  # Remove punctuation, number, and stopword tokens temporarily\n",
    "  for toks in tokenized_train_df[col]:\n",
    "    tokens.extend([t.lower() for t in toks if t.isalpha() and t.lower() not in sww])\n",
    "  wordcloud = WordCloud(background_color=\"white\").generate(\" \".join(tokens))\n",
    "  plt.figure()\n",
    "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "  plt.axis(\"off\")\n",
    "  plt.title(f\"{col.capitalize()} Word Cloud (Train Set)\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDxZSK20klWr"
   },
   "source": [
    "# 3. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ztqzVInTC1d"
   },
   "source": [
    "## 3.1 Signal tags\n",
    "\n",
    "We chose to add \\<EOS> after premise to signal the end of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fleoeIBDkr5i",
    "outputId": "b2617b00-49c9-498c-f9a1-d1453b1e257b"
   },
   "outputs": [],
   "source": [
    "def tag_seq2seq(df):\n",
    "  df = df.copy()\n",
    "\n",
    "  # Encoder input sequences\n",
    "  df[\"premise\"] = df[\"premise\"].apply(lambda s: s + [\"<EOS>\"])\n",
    "\n",
    "  # Decoder input sequences\n",
    "  df[\"hypothesis_in\"] = df[\"hypothesis\"].apply(lambda s: [\"<BOS>\"] + s)\n",
    "\n",
    "  # Decoder target sequences for teacher forcing\n",
    "  df[\"hypothesis_tar\"] = df[\"hypothesis\"].apply(lambda s: s + [\"<EOS>\"])\n",
    "\n",
    "  return df\n",
    "\n",
    "# Reorder columns\n",
    "order = [\"premise\",\"hypothesis_in\",\"hypothesis_tar\",\"label\"]\n",
    "\n",
    "train_df = tag_seq2seq(tokenized_train_df)[order]\n",
    "val_df = tag_seq2seq(tokenized_val_df)[order]\n",
    "test_df = tag_seq2seq(tokenized_test_df)[order]\n",
    "\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoSbUcLGaURF"
   },
   "source": [
    "## 3.2 Token vocabulary\n",
    "\n",
    "The vocabulary only includes tokens in train set with `frequency >= 3`.\n",
    "\n",
    "Therefore, a token with low frequency will be regarded as \\<UNK>.\n",
    "\n",
    "E.g. the token \"6.39\" from the first premise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6MnwSgIVx58",
    "outputId": "ee61ec4a-c01f-4e1e-b60f-9457fcb851a6"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(tagged_df, min_freq=3):\n",
    "  counter = Counter()\n",
    "  for col in [\"premise\", \"hypothesis\"]:\n",
    "    for toks in tagged_df[col]:\n",
    "      counter.update(toks)\n",
    "\n",
    "  # special tokens\n",
    "  vocab = {\"<PAD>\": 0,\"<UNK>\": 1,\"<BOS>\": 2,\"<EOS>\": 3}\n",
    "  for token, freq in counter.items():\n",
    "    if freq >= min_freq and token not in vocab:\n",
    "      vocab[token] = len(vocab)\n",
    "\n",
    "  return vocab\n",
    "\n",
    "vocab = build_vocab(tokenized_train_df)\n",
    "print(\"Vocab size:\", {len(vocab)})\n",
    "print(\"Sample tokens:\", list(vocab.keys())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVw81ZCbgjSh"
   },
   "source": [
    "## 3.3 One-hot key embedding\n",
    "\n",
    "Map each token to its one-hot key from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gg2wWyg-bV3r",
    "outputId": "8932d6c0-a71c-4c83-9953-3a20e200c7ce"
   },
   "outputs": [],
   "source": [
    "def word_to_index(df, vocab):\n",
    "  df = df.copy()\n",
    "\n",
    "  for col in [\"premise\", \"hypothesis_in\", \"hypothesis_tar\"]:\n",
    "    df[col] = df[col].apply(lambda toks: [vocab.get(w, vocab[\"<UNK>\"]) for w in toks])\n",
    "\n",
    "  label_map = {\"neutral\": 0, \"entails\": 1}\n",
    "  if \"label\" in df.columns:\n",
    "    df[\"label\"] = df[\"label\"].map(label_map).fillna(-1).astype(int)\n",
    "\n",
    "  return df\n",
    "\n",
    "# One-hot key embedding\n",
    "indexed_train_df = word_to_index(train_df, vocab)\n",
    "indexed_val_df = word_to_index(val_df, vocab)\n",
    "indexed_test_df = word_to_index(test_df, vocab)\n",
    "\n",
    "display(indexed_train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZJlo-z8k0ai"
   },
   "source": [
    "# 4. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTHB84RcbOFo"
   },
   "source": [
    "## 4.1 Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mtl8jwlgCpu3"
   },
   "source": [
    "### 4.1.1 Enable cuda GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9W2gHTTCyM2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyhnsNdv7kVj"
   },
   "source": [
    "### 4.1.2 Shared hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgNqexCv7fHf"
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "shared = SimpleNamespace(\n",
    "  vocab_size = len(vocab),\n",
    "  embed_dim = 256,\n",
    "  hidden_dim = 256,\n",
    "  num_layers = None,      # number of recurrent layers differs across models\n",
    "  dropout = 0.1,\n",
    "  batch_size = 128,\n",
    "  learning_rate = 1e-3,\n",
    "  total_epoch = 20,\n",
    "  pad_idx = vocab[\"<PAD>\"],\n",
    "  unk_idx = vocab[\"<UNK>\"],\n",
    "  bos_idx = vocab[\"<BOS>\"],\n",
    "  eos_idx = vocab[\"<EOS>\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NWYbjTKb7RE"
   },
   "source": [
    "### 4.1.3 Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3P_ZR8E1b-j4",
    "outputId": "8a6d061f-576d-4b0a-8a51-350394c6b38b"
   },
   "outputs": [],
   "source": [
    "def pad(seq, max_len):\n",
    "  return seq + [shared.pad_idx] * (max_len - len(seq))\n",
    "\n",
    "def pad_seq(df):\n",
    "  df = df.copy()\n",
    "\n",
    "  max_prem = max(df[\"premise\"].apply(len))\n",
    "  max_hypo_in = max(df[\"hypothesis_in\"].apply(len))\n",
    "  max_hypo_tar = max(df[\"hypothesis_tar\"].apply(len))\n",
    "\n",
    "  df[\"premise\"] = df[\"premise\"].apply(lambda s: pad(s, max_prem))\n",
    "  df[\"hypothesis_in\"] = df[\"hypothesis_in\"].apply(lambda s: pad(s, max_hypo_in))\n",
    "  df[\"hypothesis_tar\"] = df[\"hypothesis_tar\"].apply(lambda s: pad(s, max_hypo_tar))\n",
    "\n",
    "  return df\n",
    "\n",
    "# Padding\n",
    "train_df = pad_seq(indexed_train_df)\n",
    "val_df = pad_seq(indexed_val_df)\n",
    "test_df = pad_seq(indexed_test_df)\n",
    "\n",
    "# Verification\n",
    "pd.set_option(\"display.max_colwidth\", 60)\n",
    "display(train_df.head())\n",
    "pd.reset_option(\"display.max_colwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6Gm9F5Mb-0p"
   },
   "source": [
    "### 4.1.4 Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6cJWrEacAhH"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def make_batch(idx_df):\n",
    "  batch_size = shared.batch_size\n",
    "\n",
    "  encoder_inputs = torch.tensor(idx_df[\"premise\"].tolist())\n",
    "  decoder_inputs = torch.tensor(idx_df[\"hypothesis_in\"].tolist())\n",
    "  targets = torch.tensor(idx_df[\"hypothesis_tar\"].tolist())\n",
    "  labels = torch.tensor(idx_df[\"label\"].tolist()) if \"label\" in idx_df else None\n",
    "\n",
    "  dataset = TensorDataset(encoder_inputs, decoder_inputs, targets, labels)\n",
    "  loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67TQJgOJ_lF1"
   },
   "source": [
    "## 4.2 The vanilla RNN encoder-decoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaiNx53_Sxia"
   },
   "source": [
    "* Single recurrent layer for both encoder and decoder. (Inactive dropout between RNN layers)\n",
    "* Dropout applied to endocer hidden state and decoder output\n",
    "* Teacher forcing strategy during training\n",
    "* No attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6msx2of6C8_U"
   },
   "source": [
    "### 4.2.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoCjPNp8C-b4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN_Seq2Seq_Model(nn.Module):\n",
    "  def __init__(self, shared):\n",
    "    super().__init__()\n",
    "\n",
    "    # Specialized hyperparameters\n",
    "    self.num_layers = 1\n",
    "\n",
    "    # Layer definitions\n",
    "    self.embedding = nn.Embedding(shared.vocab_size, shared.embed_dim, padding_idx=shared.pad_idx)\n",
    "\n",
    "    self.encoder = nn.RNN(\n",
    "      shared.embed_dim, shared.hidden_dim,\n",
    "      num_layers=self.num_layers,\n",
    "      # dropout=shared.dropout,   # Dropout only applies between RNN layers\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "    self.decoder = nn.RNN(\n",
    "      shared.embed_dim, shared.hidden_dim,\n",
    "      num_layers=self.num_layers,\n",
    "      # dropout=shared.dropout,   # Dropout only applies between RNN layers\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "    # Prevents overfitting\n",
    "    self.dropout_encoder = nn.Dropout(shared.dropout)\n",
    "    self.dropout_decoder = nn.Dropout(shared.dropout)\n",
    "\n",
    "    self.linear = nn.Linear(shared.hidden_dim, shared.vocab_size)\n",
    "\n",
    "  def forward(self, enc_in, dec_in):\n",
    "    # Encoder\n",
    "    enc_embed = self.embedding(enc_in)        # encoder embedding\n",
    "    _, hidden = self.encoder(enc_embed)       # discard encoder output\n",
    "    hidden = self.dropout_encoder(hidden)      # apply dropout layer to hidden state\n",
    "\n",
    "    # Decoder\n",
    "    dec_embed = self.embedding(dec_in)        # decoder embedding\n",
    "    dec_out, _ = self.decoder(dec_embed, hidden)  # inherit hidden state, discard decoder final hidden state\n",
    "    dec_out = self.dropout_decoder(dec_out)     # apply dropout layer to decoder output\n",
    "\n",
    "    logits = self.linear(dec_out)          # decoder raw prediction\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diVN3tW6qmTL"
   },
   "source": [
    "### 4.2.2 Training (2 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQqFb5Csqv03"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = RNN_Seq2Seq_Model(shared).to(device)\n",
    "criterion = nn.CrossEntropyLoss()     # combine LogSoftmax and NLLLoss\n",
    "optimizer = optim.Adam(model.parameters(), lr=shared.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHXsq4Fl_bmg"
   },
   "outputs": [],
   "source": [
    "train_loader = make_batch(train_df)\n",
    "\n",
    "for encoder_batch, decoder_batch, target_batch, _ in train_loader:  # discard label, because we only concern about how to generate a sequence\n",
    "  # Move to device\n",
    "  encoder_batch = encoder_batch.to(device)\n",
    "  decoder_batch = decoder_batch.to(device)\n",
    "  target_batch = target_batch.to(device)\n",
    "\n",
    "  # Forward pass\n",
    "  logits = model(encoder_batch, decoder_batch)\n",
    "  loss = criterion(logits.view(-1, logits.size(-1)), target_batch.view(-1))\n",
    "\n",
    "  # Backward and optimize\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8hdF7T0pBxU2",
    "outputId": "eb950107-c070-425c-9afb-b2704e7e7ccb"
   },
   "outputs": [],
   "source": [
    "train_epoch_losses = []\n",
    "\n",
    "for epoch in range(shared.total_epoch):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "\n",
    "  for enc, dec, tar, _ in train_loader:\n",
    "    enc, dec, tar = enc.to(device), dec.to(device), tar.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(enc, dec)    # enable teacher forcing\n",
    "    loss = criterion(logits.view(-1, logits.size(-1)), tar.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "  train_epoch_losses.append(avg_loss)\n",
    "  print(f\"Epoch [{epoch+1}/{shared.total_epoch}]， Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# TODO: unused validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPkVjEM4LNu-"
   },
   "source": [
    "### 4.2.3 Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "hznRuf-PLKk9",
    "outputId": "214580f8-9800-4702-ef26-2f27c3de9910"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch = range(1, shared.total_epoch + 1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.plot(epoch, train_epoch_losses, marker='o')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMb7H46HOLqG"
   },
   "source": [
    "### 4.2.4 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XOEgZPGOivL"
   },
   "source": [
    "### 4.2.5 Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhSE5ON4_r0C"
   },
   "source": [
    "## 4.3 The Bi-LSTM encoder-decoder with XXX-attention in different positions\n",
    "\n",
    "Lab 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b14s1AoN_wNC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4w_n2P2_xxm"
   },
   "source": [
    "## 4.4 The vanilla Transformer with self-attention\n",
    "\n",
    "Lab 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpZNRYhe_1ae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GU_rQZJk87q"
   },
   "source": [
    "# 5. Performance Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L11vnP-lk_ZH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zvj4_3Qak_x7"
   },
   "source": [
    "# 6. Interactive Inference Colab Form\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMJN1Dx1lES9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3tgj4JarkjAY"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
